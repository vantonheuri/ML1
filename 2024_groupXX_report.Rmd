---
title: |
  ![](LogoZugEstates.png){width=4in}   
  Real Estate Pricing
author: "Victor Anton, Rodrigo Gonzalez & Carlos Leon"
date: "Machine Learning 1 - 05 June 2024"
output: 
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
    #highlight: tango
    toc: yes
    toc_depth: 1
    # toc_float: yes
    number_sections: yes
    #code_folding: hide
  pdf_document:
    toc: yes
    toc_depth: 3
  # html_document:
  #   toc: yes
  #   toc_depth: 2
  #   toc_float: true
  #   number_sections: yes
  #   code_folding: hide
  #   theme: cerulean
  #   highlight: tango
knit:
  citations: yes
---

```{r setup, include = FALSE, warning = FALSE}
library(knitr)
opts_chunk$set(
  echo = TRUE,
  eval = TRUE,
  warning = FALSE,
  message = FALSE)
```

# Introduction

Zug Estates focuses on developing, managing, and marketing properties in the Zug region, Switzerland. They prioritize centrally located sites for sustainable, multi-use development. Their portfolio is concentrated in Zug and Risch Rotkreuz, featuring a mix of residential, office, retail, hotel, and service spaces. The company aims for long-term property retention and development, driving value creation through sustainable designs and active management

Zug Estates, a major real estate player in Zug, Switzerland, might be looking to invest in other parts of the country for several reasons. Here's a comprehensive breakdown:

-   **Risk Reduction:** The real estate market can be volatile. By spreading their investments across different cantons (Swiss states), Zug Estates reduces the risk associated with a downturn in Zug's market. They might target areas with strong growth potential or a different property mix to complement their Zug portfolio. This diversification provides a buffer and ensures a more stable income stream.

-   **Saturated Market:** Perhaps the Zug market is saturated, meaning there are limited opportunities for profitable investments. Expanding to other areas allows Zug Estates to tap into new markets with higher potential returns. They might find regions with a greater need for the kind of development they specialize in, offering exciting growth possibilities.

-   **Connecting the Landscape:** Zug Estates' investments might have a strategic element beyond just individual properties. They could be acquiring properties near transportation hubs or in developing commercial centers. This could be a way to connect different areas of Switzerland through their investments, potentially influencing the overall development landscape.

-   **Reputation and Knowledge:** Over time, Zug Estates has likely built a strong reputation for high-quality development and property management within Zug. Expanding to other cantons allows them to leverage this reputation. They can attract partnerships with local developers or win contracts for projects in new areas based on their proven track record. Essentially, they bring their successful model to other parts of Switzerland.

-   **Capitalizing on Demand:** Zug Estates might be strategically responding to broader market trends across Switzerland. Perhaps there's a growing demand for specific property types, like student housing or retirement communities, in certain regions. By investing in these areas, they can capitalize on these trends and meet those demands, ensuring their portfolio remains relevant and profitable. .

## Dataset "Homegate..."

... Rodrigo ...

## Motivation and Goal

... Rodrigo ...

## Project Structure

... Rodrigo ...

## Navigating the Report: Understanding the Hidden R Code

This report is based on R code that was used to explore, train, and test various models on the data. To maintain a clean and accessible format, much of this code is hidden within the document. However, upon request, the code can be made visible for thorough review. This feature not only helps ensure the reproducibility of our analysis but also facilitates ease of reading.

# Data Preparation

Before proceeding to the exploratory graphical analysis, this chapter provides a brief overview of how the dataset was loaded and prepared.

## Libraries

In this report the following libraries are used.

<details>

<summary>*Click to see all libraries*</summary>

```{r libraries, class.source = "fold-show"}
# used libraries
library(readr)
library(ggplot2)
library(stringr)
library(dplyr)
library(readxl)
library(openxlsx)
library(lubridate)
library(readxl)
library(neuralnet)
library(caret)
library(e1071)
```

</details>

<br>

# Exploratory Graphical Analysis

This chapter delves into an in-depth exploration of the dataset to gain a comprehensive understanding of the underlying data, with a particular focus on the key variable of interest: attrition. As highlighted in the introduction, our primary objective in this section is to examine attrition closely and to analyze its relationships with other variables in the dataset.

A critical observation from our initial analysis is the notable imbalance in the response variable. The data reveals that a significant majority of the workforce, amounting to over 80%, have not experienced attrition. Specifically, out of the total number of employees in the dataset, 1233 remain employed, while 237 have left the company (marked as 'attrition = yes'). This disproportionate representation poses a potential challenge in the context of predictive modeling. Models developed under these conditions may inherently exhibit a bias towards the majority class, in this case, the employees who have stayed.

```{r, echo=FALSE}
df_total <- read_excel("data/data_cleaned/data_total.xlsx")
colnames(df_total)
```

### Properties per Canton

Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.

```{r Properties per Canton, echo=FALSE, warning = FALSE}
# Counting properties per Canton and arranging them from highest to lowest
properties_per_canton <- df_total %>%
  group_by(Canton) %>%
  summarise(Count = n()) %>%
  arrange(desc(Count)) # Arrange in descending order of count

# Creating the vertical bar chart with smaller data labels
ggplot(properties_per_canton, aes(x = reorder(Canton, -Count), y = Count)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  geom_text(aes(label = Count), vjust = -0.5, color = "black", size = 2.2) + # Data labels above bars
  labs(title = "Properties per Canton", x = "Canton", y = "Count of Properties") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), # Rotate x-axis labels for better readability
        plot.title = element_text(hjust = 0.5))
```

### Average Rental Price per Canton

Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.

```{r Average Rental Price per Canton, echo=FALSE, warning = FALSE}
canton_order <- df_total %>%
  group_by(Canton) %>%
  summarise(Average_Price_Gross = mean(Price_Gross, na.rm = TRUE)) %>%
  arrange(desc(Average_Price_Gross)) %>%
  .$Canton

# Adjust the factor levels of Canton based on the calculated order
df_total$Canton <- factor(df_total$Canton, levels = canton_order)

# Plot with cantons ordered by average rental price
ggplot(df_total, aes(x = Canton, y = Price_Gross)) +
  stat_summary(fun = "mean", geom = "bar", fill = "skyblue", color = "black") +
  labs(title = "Average Rental Price per Canton", x = "Canton", y = "Average Rental Price (CHF)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

### Differences in rental prices between different customer segment

Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.

```{r Differences in rental prices between different customer segment, echo=FALSE, warning = FALSE}
  # - Are there differences in rental prices between different customer segments?
  # Option 1
  ggplot(df_total, aes(x = Customer_Segment, y = Price_Gross, fill = Customer_Segment)) +
  geom_boxplot(outlier.size = 1.5, alpha = 0.7) + 
  scale_fill_brewer(palette = "Pastel1") + 
  coord_cartesian(ylim = c(0, quantile(df_total$Price_Gross, 0.95, na.rm = TRUE))) + 
  labs(title = "Rental Prices by Customer Segment", x = "Customer Segment", y = "Gross Price (CHF)") +
  theme_minimal() +
  theme(legend.position = "none") 

# Option 2
ggplot(df_total, aes(x = Customer_Segment, y = Price_Gross)) +
  geom_boxplot(outlier.size = 1, aes(fill = Customer_Segment)) + 
  scale_y_log10(limits = c(100, NA)) + 
  scale_fill_brewer(palette = "Pastel1", guide = FALSE) + 
  labs(title = "Rental Prices by Customer Segment", x = "Customer Segment", y = "Gross Price (CHF)") +
  theme_minimal() +
  theme(legend.position = "none")
```

### TITULO

Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.

```{r Correlation Between Property Size and Rental Price, echo=FALSE, warning = FALSE}
# - Is there a correlation between the size of the property (in m2) and the rental price?
  
# We sleect the Top Ten Cantons
top_10_cantons <- c("ZH", "VD", "AG", "SG", "BE", "TI", "LU", "TG", "BS", "ZG")
top_5_cantons <- c("ZH", "VD", "AG", "SG", "BE")

# Filter the dataset to include only the top cantons
df_top_10_cantons <- df_total %>%
  filter(Canton %in% top_10_cantons) %>%
  filter(Price_Gross <= 500000, Size_m2 <= 300) # Apply filters for Price_Gross and Size_m2

# Filter the dataset to include only the top cantons
df_top_5_cantons <- df_total %>%
  filter(Canton %in% top_5_cantons) %>%
  filter(Price_Gross <= 500000, Size_m2 <= 300) # Apply filters for Price_Gross and Size_m2

# Create a manual pastel color palette
pastel_colors <- c("ZH" = "#bdb2ff",  # Pastel blue
                   "VD" = "#ffd6a5",  # Lighter pastel blue
                   "AG" = "#b4f8c8",  # Pastel cyan
                   "SG" = "#ade8f4",  # Lighter pastel cyan
                   "BE" = "#ffafcc",  # Pastel red
                   "TI" = "#bde0fe",  # Pastel orange
                   "LU" = "#90e0ef",  # Pastel green
                   "TG" = "#a0c4ff",  # Another shade of pastel blue
                   "BS" = "#a1c9f4",  # Pastel purple
                   "ZG" = "#ffc6ff") # Pastel pink

# Scatter Plot TOP 5
ggplot(df_top_5_cantons, aes(x = Size_m2, y = Price_Gross)) +
  geom_point(aes(color = Canton), size = 0.8, alpha = 0.8) + # Smaller points with size 1
  scale_color_manual(values = pastel_colors) +
  geom_smooth(method = "lm", color = "darkblue", size = 0.5, se = FALSE) +
  labs(title = "Correlation Between Property Size and Rental Price", x = "Size (m²)", y = "Gross Price (CHF)") +
  theme_minimal() +
  scale_y_log10(labels = scales::comma) # Apply logarithmic scale to y-axis
```

### Distribution of Property Size

Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.

```{r Distribution of Property Size, echo=FALSE, warning = FALSE}
# Histogram for Size_m2
ggplot(df_total, aes(x = Size_m2)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Property Size", x = "Size (m²)", y = "Frequency") +
  theme_minimal()
```

### Distribution of Number of Rooms

Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.

```{r Distribution of Number of Rooms, echo=FALSE, warning = FALSE}
# Histogram for Nr_rooms
ggplot(df_total, aes(x = Nr_rooms)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Number of Rooms", x = "Number of Rooms", y = "Frequency") +
  theme_minimal()
```

*Interpreta el grafico*

### Effect of Number of Rooms on Rental Price

Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.

```{r Effect of Number of Rooms on Rental Price, echo=FALSE, warning = FALSE}
# - How does the number of rooms affect the rental price?

# Apply filters to remove extreme prices for better visualization
  df_filtered <- df_total %>%
  filter(Price_Gross <= 500000) %>%
  mutate(Nr_rooms = as.factor(Nr_rooms)) # Treat number of rooms as a factor

# Create the scatter plot without differentiating by canton
ggplot(df_filtered, aes(x = Nr_rooms, y = Price_Gross)) +
  geom_point(alpha = 0.6, size = 1, color = "skyblue") +
  labs(title = "Effect of Number of Rooms on Rental Price", x = "Number of Rooms", y = "Gross Price (CHF)") +
  theme_minimal() +
  scale_y_log10() # Apply logarithmic scale to y-axis
```

# Linear Model

```{r Linear Model, echo=FALSE, warning=FALSE}
# Linear Model
# Target Variable: Price_Gross
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Required Packages
library(readxl)
library(ggplot2)
#---------------------------------------------------------------------------
set.seed(71)
# Reading in data
enc_data <- read_excel("data/data_cleaned/data_total_model.xlsx")

#Splitting the data into training and testing
split_ratio <- 0.8
training_indices <- sample(1:nrow(enc_data), 
                           size=nrow(enc_data) * split_ratio,
                           replace=FALSE)

train_set <- enc_data[training_indices,]
test_set <- enc_data[-training_indices,]

#Fit the model    data[, !(names(data) %in% column_to_exclude)]
all_lm_model <- lm(Price_Gross ~ ., data=train_set)

# Retraining with only relevant variables
rel_data <- enc_data[, !(names(enc_data) %in% 
                           c("Package_Product_num",
                             "Type_num",
                             "GDP_2020_21","GDP_per",
                             "Population",
                              "Area_km2", "Density")), drop = TRUE]

rel_train_set <- rel_data[training_indices,]
rel_test_set <- rel_data[-training_indices,]

# Fit the model
few_lm_model <- lm(Price_Gross ~ ., data=rel_train_set)

# Retraining with fewer variables
simp_data <- enc_data[, c("Nr_rooms", "Category_num", "Price_Gross")]

simp_train_set <- simp_data[training_indices,]
simp_test_set <- simp_data[-training_indices,]

# Fit the model
simple_lm_model <- lm(Price_Gross ~ ., data=simp_train_set)

#Model Predictions
all_lm_predictions <- predict(all_lm_model, newdata=test_set)
few_lm_predictions <- predict(few_lm_model, 
                              newdata=rel_test_set)
simple_lm_predictions <- predict(simple_lm_model, 
                                 newdata=simp_test_set)

#Model Evaluation

rmse <- sqrt(mean((all_lm_predictions - test_set$Price_Gross)^2))
mae <- mean(abs(all_lm_predictions - test_set$Price_Gross))

new_rmse <- sqrt(mean((few_lm_predictions - rel_test_set$Price_Gross)^2))
new_mae <- mean(abs(few_lm_predictions - rel_test_set$Price_Gross))

simp_rmse <- sqrt(mean((simple_lm_predictions - simp_test_set$Price_Gross)^2))
simp_mae <- mean(abs(simple_lm_predictions - simp_test_set$Price_Gross))

```

## Purpose and Target

We decided to employ a linear regression model in order to predict prices for different rental properties, specifically apartments. We created three different models, each taking into account different features to help us gain better insight.

### Linear Model 1

The first model (*all_lm_model*) takes all available property features into account to train and make predictions. From this we were able to see that some of those we included were not relevant to the target variable, as seen below:

```{r First Linear model, echo=FALSE}
summary(all_lm_model)
```

*As we can see in the model summary, some variables are converted into factors and interpreted separately, we can also see 'Nr_rooms' is relevant in general, but Package_Product and Category, for example, are not.*

With this information we were able to refine our selection of variables for training the following models, one with only a few variables, and one with only one.

### Linear Model 2

The next model ( *few_lm_model* ) we trained with fewer dependent variables, only those of high importance. For this one, we took into account only the ones that were calculated to be significant to the model's performance.

```{r Few Model Summary, echo=FALSE}
summary(few_lm_model)
```

*Here we can see that the variables taken into account were Canton, Days_Difference, Category, Customer_Segment, Nr_rooms and Size_m2.*

### Linear Model 3

The last model generated took into account only *two* variable as a predictor: *Nr_rooms* and *Category_num2*. The number of rooms seems to have one of, if not the biggest effect on rental price of a property, which is why we went with this variable for analysis. The second customer category also has a significant effect, so it was included.

```{r Simple Model Summary, echo=FALSE}
summary(simple_lm_model)$coef
```

## Interpretation

All three models offer us some insight into the behavior of rental prices for different properties. Interestingly, our research thus far shows that the Canton of said property is not that influential in its rental price, except for Canton 3.

*(Canton 1: Argau, Canton 2: Luzern, Canton 3: Zürich, Canton 4: Zug)*

For desired results, we will focus on the second model, which includes all the variables considered significant for the best results.

```{r Few Model Plot, echo=FALSE}
lm_plot1 <- data.frame(rel_data$Nr_rooms, rel_data$Price_Gross)
lm_plot2 <- data.frame(rel_data$Category_num, rel_data$Price_Gross)
plot(rel_data$Category_num, rel_data$Price_Gross, xlab="Category", ylab="Price_Gross", main="Price of a Rental Property as a Result of the Customer Category")

```

In this diagram we can observe that, though there are fewer instances of Category 2 (House) compared to Category 1 (Apartment), Category 2 properties tend to have higher rental prices. This does not, however, indicate that they may be the more lucrative option, as houses tend to incur other costs that are either not present, or the cost is split in the community, in the case of apartments.

```{r First Model Plot, echo=FALSE}

plot(lm_plot1, xlab="Nr_Rooms", ylab="Price_Gross", main="Price of a Rental Property as a result of the Number of Rooms")

```

In the illustration above, we depict the relationship between the number of rooms in a property and the gross price of the same. We can observe an upward trend, which leads to believe that a higher number of rooms leads to a higher price, which would make sense. This information must also be taken into consideration together with other variables, like the property's area in meters squared, average size of rooms, location, etc.

## Conclusion

Our linear models suggest that properties, specifically houses, with more rooms go for a higher price. This doesn't necessarily indicate that they may be the best choice for renting out to customers, but it does suggest that they are the properties which generate the highest gross payment on a regular basis.

# Neural Network

```{r NeuralNet - Base, echo=FALSE}
# Required Packages
library(readxl)
library(neuralnet)

# Reading in data
enc_data <- read_excel("data/data_cleaned/data_total_model.xlsx")
enc_data <- enc_data[, !(names(enc_data) %in% "X"), drop = TRUE]

# Feature Engineering
enc_data$Price_per_m2 <- enc_data$Price_Gross / enc_data$Size_m2
q3 <- median(enc_data$Price_per_m2) + IQR(enc_data$Price_per_m2) / 2
enc_data$High_Ticket <- enc_data$Price_per_m2 > q3
enc_data$High_Ticket <- ifelse(enc_data$High_Ticket, 1, 0)

# Declaring Categorical Variables
enc_data$Canton_num <- as.numeric(as.factor(enc_data$Canton_num))
enc_data$Customer_Segment_num <- as.numeric(as.factor(enc_data$Customer_Segment_num))
enc_data$Category_num <- as.numeric(as.factor(enc_data$Category_num))
enc_data$Nr_rooms <- as.numeric(enc_data$Nr_rooms) # Treating Nr_rooms as continuous
enc_data$Package_Product_num <- as.numeric(as.factor(enc_data$Package_Product_num))

# Normalizing the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

features <- subset(enc_data, select = -High_Ticket)
maxmindf_features <- as.data.frame(lapply(features, normalize))

High_Ticket <- enc_data$High_Ticket
maxmindf <- cbind(High_Ticket, maxmindf_features)

# Split into train and test sets
set.seed(36)
smpl <- sample.int(n = nrow(maxmindf), size = floor(0.8 * nrow(maxmindf)), replace = FALSE)
train_maxmin <- maxmindf[smpl, ]
test_maxmin <- maxmindf[-smpl, ]

# Modeling
model_maxmin <- neuralnet(High_Ticket ~ ., data = train_maxmin, hidden = c(3, 8, 5), linear.output = FALSE)

# Predicting
pred_maxmin <- predict(model_maxmin, test_maxmin)

# Testing the Accuracy of the models
maxmin_results <- data.frame(actual = test_maxmin$High_Ticket, prediction = pred_maxmin)

```

## Purpose and Target

When considering using a Neural Network, we decided to identify potential high value properties within the dataset. To do this, we engineered a new, binary column, identifying whether a property was considered "High-Ticket" or not. In order to categorize properties this way, we calculated the price per meter squared of each property, and those in the top quartile had their High_Ticket column value set to 1, others to 0.

With this transformation, we had created a binary classification problem: is this property a high-ticket property? Answering this question will allow us to filter out most properties which may not be as lucrative for Zug Estates, and offer only the cream of the crop as possible targets for acquisition.

## Findings

After classifying the properties into either category, we were able to analyze the model's performance on the dataset. If successful, this Neural Network will be a great tool in the future to predict a possible acquisition's performance over time. Properties may be acquired at lower prices, and may not be very attractive at the time of purchase, but with some remodeling and updating the living space, its perceived value may be brought to a new high. This is what we try to predict with this model: the potential of a specific property and to discern if it could be a lucrative investment for Zug Estates.

```{r NN Findings, echo=FALSE}
# Confusion matrix
roundedresults <- sapply(maxmin_results, round, digits = 0)
roundedresultsdf <- data.frame(roundedresults)
nn_conf <- confusionMatrix(as.factor(roundedresultsdf$prediction),
                           as.factor(roundedresultsdf$actual),
                           dnn=c("Prediction", "Reference"))
nn_conf$table
```

*Confusion Matrix of the Neural Network's performance.*

As seen in the previous matrix, the model's performance on categorizing properties into possible high-ticket or not is quite good. This tool will help in the future, when looking at new properties, to have an idea of more or less how beneficial a specific property could be to Zug Estates' portfolio.

```{r Predictions Chart, echo=FALSE}
# Convert to a data frame for ggplot2
rounded_predictions <- round(pred_maxmin)
data <- data.frame(predictions = factor(rounded_predictions, levels = c(0, 1), labels = c("Non-High Ticket", "High-Ticket")))


# Create the histogram
ggplot(data, aes(x = predictions, fill = predictions)) +
  geom_bar(color = "black", width = 0.5) +
  scale_x_discrete(name = "Predicted Class") +
  ylab("Frequency") +
  ggtitle("Rounded Predictions") +
  scale_fill_manual(values = c("Non-High Ticket" = "skyblue", "High-Ticket" = "#b4f8c8"))

```

*Predictions have been rounded because some values fell into decimal places, due to the nature of the Neural Networks' activation function (sigmoid curve)*

As we can see, not many values actually fall into the category of High-Ticket properties, so having a tool like this network will be beneficial in catching such opportunities early on and maximizing added value and profits for the real estate company.

# Support Vector Machine

Though the Neural Network presented great results, we wanted to make sure to validate that it was the optimal choice for classification in our problem. Support Vector Machines (SVMs henceforth) also excel at binary classification problems, so we wanted to create one and train it on the same data in order to visualize and compare performance, to ensure we use the better performing model for recommending properties to acquire.

```{r SVM Intro, echo=FALSE, results='hide'}

# Reading in data
ohe_data <- read_excel("data/data_cleaned/data_total_model_one_hot_encoded.xlsx")

#Feature Engineering
ohe_data$Price_per_m2 <- ohe_data$Price_Gross / ohe_data$Size_m2
q3 <- median(ohe_data$Price_per_m2) + IQR(ohe_data$Price_per_m2) / 2
ohe_data$High_Ticket <- ohe_data$Price_per_m2 > q3
ohe_data$High_Ticket <- ifelse(ohe_data$High_Ticket, 1, 0)

# Price_per_m2 causes collinearity issues
# Nr_rooms.8 has only one positive value, so we remove the row
# Nr_rooms.10 is constant, so we remove it as well
ohe_data <- ohe_data[, !names(ohe_data) %in% c("Price_per_m2","Nr_rooms.8",
                                               "Nr_rooms.10")]

# Convert target to factor since this is a classification problem (binary)
ohe_data$High_Ticket <- as.factor(ohe_data$High_Ticket)

# Random seed for reproducibility
set.seed(123) # same seed as NN for reproduction/comparison

# Split the data into training and test sets
train_index <- sample(seq_len(nrow(ohe_data)), size = floor(0.8 * nrow(ohe_data)))
train_data <- ohe_data[train_index, ]
test_data <- ohe_data[-train_index, ]

# Set up train control for cross-validation
train_control <- trainControl(
  method = "cv",         # Cross-validation
  number = 10,           # Number of folds
  savePredictions = "final",
  classProbs = TRUE,     # If you want class probabilities
  summaryFunction = twoClassSummary
)


# Train the SVM model
svm_model <- svm(High_Ticket ~ ., data = train_data,trControl=train_contor,
                 kernel = "radial",
                 cost = 10, scale = TRUE)

# Exclude the target variable from the test set
test_data_without_target <- test_data[, !names(test_data) %in% 'High_Ticket']


# Make predictions
svm_predictions <- predict(svm_model, newdata = test_data_without_target)

```

```{r Re-NN Performance, echo=FALSE}
nn_conf$table
```

*Neural Network Performance*

```{r SVM Confusion Matrix, echo=FALSE}

# Calculate confusion matrix
svm_confusion <- confusionMatrix(as.factor(svm_predictions), as.factor(test_data$High_Ticket))

svm_confusion$table

```

*SVM Performance*

As we can see, comparing the two confusion matrices, the Neural Network has slightly better accuracy than the SVM. Though at first glance it may seem like the better option, there are several advantages and disadvantages between the two models.

An SVM is particularly effective when working with higher-dimensional data, and has better memory efficiency. It is also more robust to over-fitting, but are less efficient with larger datasets because of the training complexity.

Neural networks on the other hand, are more flexible and powerful when it comes to approximating trends. They can also learn and extract features that are not present in the data, finding new relationships that may not be obvious, but can be very computationally intensive.

In summary, both are good options for our purpose. In the future, if considering to use either one, the size, shape and complexity of the available dataset will play a pivotal role in the selection of the best model to use for finding possible "golden goose" properties.

## Outlook

We would like to highlight our top four recommendations for improvements in future endeavors related to this project and more broadly in the field of employee attrition.

### Data Quality and Amount

Future studies could benefit from investigating ensemble methods, which merge the advantages of various models to boost overall efficacy. We suggest that a larger dataset might enhance the performance of specific models like neural networks. Furthermore, gathering additional predictive variables could facilitate the training of more sophisticated models.

The primary focus of our project was centered on identifying general predictors of attrition. This objective shaped our analytical approach and the methodologies we employed, and provided a foundation for understanding the key predictors. An extension of our analysis would be to change the focus and predict the probabilities of employee attrition, rather than solely classifying outcomes. This approach would enable us to personalize interventions more effectively, tailoring strategies to the specific likelihood of an employee's departure.

### Enhanced Validation of Models

Due to the time constraints and scope of this project, we couldn't conduct extensive validation and testing for all models. Implementing more techniques such as cross-validation and regularization in the training phase would be necessary to enhance the overall quality and performance of the developed models. Moreover, testing interactions was at short-cut in this project. In

### Model Complexity vs. Performance

The study underscored the delicate balance between model complexity and performance. Although more sophisticated models, as seen in the GAM and SVM sections, are adept at identifying complex patterns, they don't always guarantee the most accurate predictions in every situation, particularly regarding specificity. When developing a model for a company, it's essential to clearly articulate the model's intended purpose and application to determine the most appropriate solution.

### Data Privacy and Ethics

While we have pointed out that various stakeholders can gain from these models, it's crucial to note that the personal data used here, such as marital status, is highly sensitive in non-fictional datasets. Companies should thoughtfully consider which data points to utilize about their employees. Relying on certain predictors can potentially introduce significant bias, particularly against underrepresented groups within the organization.

## Personal Learnings

This project was a great opportunity for us to learn about machine learning and develop our skills. We really enjoyed experimenting with different models, which is why our report is so long - we had a lot of work to cut down.

Our models are of solid quality, but we didn't get to explore every detail or use all the methods perfectly. For example, our early graphical analysis could be improved by using methods better suited to different types of data than just correlation coefficients, or validation of different models is not fully implemented (CSV). The neural network component, in particular, posed significant challenges. Our results here were not as robust as we had hoped, possibly due to limitations in the amount of data available. This experience highlighted the importance of having a substantial dataset for neural network models to truly excel.

An important lesson we learned was the need to establish common measures in advance for effective model comparison. This ensures that a meaningful comparison can be made using the measures. We also learned the importance of considering the data set when choosing metrics. In our case, relying solely on accuracy as a metric wasn't appropriate due to the unbalanced nature of the dataset.

It's important to note that OpenAI's Chat-GPT and Google's Gemini LLMs were valuable resources when we were faced with difficult debugging problems. However, we made a point of understanding, modifying and correcting the suggested code independently to ensure that our final work was truly our own and not a mere replication of the solutions provided.

In the end, we're happy with the end result of our report, and even happier with how much we learned along the way.

# References
