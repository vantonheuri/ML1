---
title: |
  ![](images/LogoZugEstates.png){width=4in}   
  Real Estate Pricing
author: "Victor Anton, Rodrigo Gonzalez & Carlos Leon"
date: "Machine Learning 1 - 05 June 2024"
output: 
  pdf_document:
    toc: yes
    toc_depth: 3
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
    toc: yes
    toc_depth: 1
    # toc_float: yes
    number_sections: yes
    #code_folding: hide
  # html_document:
  #   toc: yes
  #   toc_depth: 2
  #   toc_float: true
  #   number_sections: yes
  #   code_folding: hide
  #   theme: cerulean
  #   highlight: tango
knit:
  citations: yes
---

```{=html}
<style>

body {font-size: 14px;}

p {font-size: 14px;}

h1{font-size: 17pt;}

h2{font-size: 15pt;}

h3,h4,h5,h6{font-size: 14pt;}

div.r-plot {width: 60%; margin: auto;}

img {
  max-width: 60%; 
  height: auto;
  display: block;
  margin: auto;
}


table {
  font-size: 16px;
  line-height: 0.8;
}


</style>
```
```{r setup, include = FALSE, warning = FALSE}
library(knitr)
opts_chunk$set(
  echo = TRUE,
  eval = TRUE,
  warning = FALSE,
  message = FALSE)
```

<br>

# Introduction

**Zug Estates (SWX: ZUGN)** focuses on developing, managing, and marketing properties in the Zug region, Switzerland. As a company, they prioritize centrally located sites for sustainable, multi-use development.

Their portfolio is concentrated in **Zug** and **Risch Rotkreuz**, featuring a mix of **residential**, **office**, **retail**, **hotel**, and **service spaces**. The company aims for long-term property retention and development, driving value creation through sustainable designs and active management.

**Zug Estates**, a major real estate player in Zug, Switzerland, might be looking to invest in other parts of the country for several reasons. Here’s a comprehensive breakdown:

-   **Risk Reduction:** By spreading their investments across different cantons (Swiss states), Zug Estates reduces the risk associated with a downturn in Zug’s market.

-   **Saturated Market:** Zug market could be saturated, meaning there are limited opportunities for profitable investments. Expanding to other areas allows Zug Estates to tap into new markets with higher potential returns.

-   **Connecting the Landscape:** Zug Estates’ investments might have a strategic element beyond just individual properties. They could be acquiring properties near transportation hubs or in developing commercial centers.

-   **Reputation and Knowledge:** Over time, Zug Estates has built a strong reputation for high-quality development and property management within Zug. Expanding to other cantons allows them to leverage this reputation. They can attract partnerships with local developers or win contracts for projects in new areas based on their proven track record. Essentially, they bring their successful model to other parts of Switzerland.

-   **Capitalizing on Demand:** Zug Estates might be strategically responding to broader market trends across Switzerland. Perhaps there’s a growing demand for specific property types, like student housing or retirement communities, in certain regions.

We will focus on satisfying Zug Estates as a client looking to expand their operations.

## Motivation and Goal

**WP Carey**, a prominent **Real Estate Investment Trust (REIT)** known for its diversified portfolio of commercial properties, has recently announced its strategic decision to **exit the Swiss market**. This move marks a significant shift in the company’s investment strategy, focusing more on markets where they see greater **potential for growth and stability**. As part of this exit, **WP Carey** has successfully negotiated the sale of its assets located in central Switzerland to **Zug Estates**, a leading Swiss real estate company with a strong presence in the Zug region.

<br> ![](images/wp%20carey.JPG){width="4in"} <br>

The **assets sold** include a mix of **high-value commercial properties**, **office buildings**, and **mixed-use developments** that have been integral to **WP Carey’s Swiss portfolio**. This transaction allows **WP Carey** to reallocate capital towards more lucrative opportunities in other markets, while Zug Estates stands to benefit significantly from the acquisition. By integrating these prime assets into their existing portfolio, **Zug Estates** can enhance its market position and leverage the strategic locations of these properties to **attract new tenants** and **increase rental income**. This acquisition underscores **Zug Estates** commitment to expanding its footprint in central Switzerland and reinforces its reputation as a **key player** in the **Swiss Real Estate market**.

<br> ![](images/swissinfo.JPG){width="10in"} <br>

Our team is deeply motivated to help Zug Estates by enhancing our understanding of the real estate market in Central Switzerland. The company's strategic approach has always attracted our attention, given our recurring interest in real estate investment. We believe that the future of Central Switzerland and Zug is bright, and this project represents a unique opportunity to contribute to that growth. By leveraging our collective expertise, we aim to provide valuable insights that will help maximize the potential of these newly acquired assets.

For our analysis we will focus on residential real estate due to the data limitations.

**Disclaimer:** *This is a fictional situation created to demonstrate students' ability to work with real datasets in situations that could occur in the real world.*

## Dataset “Homegate” and “Federal Statistic Office”

We used two data sources for this study:

First, **Homegate**, a **leading Swiss Real Estate platform**, has long been the go-to resource for individuals and businesses looking to buy, sell, or rent properties in Switzerland. Known for its extensive listings and user-friendly interface, **Homegate** provides detailed information on a wide range of properties across the country.

<br> ![](images/homegate.JPG){width="4in"} <br>

We have obtained a comprehensive dataset of properties listed on Homegate, which will be invaluable for Zug Estates in evaluating these assets and determining the best use for them. This dataset includes detailed characteristics of the properties such as **Property_ID**, **FirstDay_Online**, **LastDay_Online**, **Type**, **Customer_ID**, **Customer_Segment**, **Category**, **Price_Gross**, **Size_m2**, **Canton**, **Nr_rooms**, **Package_ID**, and **Package_Product**.

This private data has been obtained through legitimate channels and not via scraping, ensuring their accuracy and reliability. With this rich dataset, we will perform an in-depth analysis for Zug Estates to help them understand market trends, property performance, and customer preferences.

Second, **the Federal Statistical Office (FSO) in Switzerland** offers a wealth of regional statistics that are essential for understanding the diverse characteristics of the country's cantons.

<br> ![](images/federal.JPG){width="14in"} <br>

Among these resources are the regional portraits and key figures, which provide detailed portraits of each canton. These profiles include crucial data points such as **Canton**, **Canton_Name**, **Canton_Capital**, **GDP_2020_21**, **GDP_per**, Population, **Area_km2**, **Density**, **Official_Language_1**, **Official_Language_2**, and **Official_Language_3.** This information is invaluable for stakeholders looking to gain insights into the economic, demographic, and cultural landscape of Switzerland.

## Project Structure

*Complete*

## Navigating the Report: Understanding the Hidden R Code

This report is based on R code that was used to explore, train, and test various models on the data. To maintain a clean and accessible format, much of this code is hidden within the document.

However, upon request, the code can be made visible for thorough review. This feature not only helps ensure the reproducibility of our analysis but also facilitates ease of reading.

# Data Preparation

Before proceeding to the exploratory graphical analysis, this chapter provides a brief overview of how the dataset was loaded and prepared.

## Libraries

In this report the following libraries are used.

<details>

<summary>*Click to see all libraries*</summary>

```{r libraries, class.source = "fold-show"}
# used libraries
library(readr)
library(ggplot2)
library(stringr)
library(dplyr)
library(readxl)
library(openxlsx)
library(lubridate)
library(readxl)
library(neuralnet)
library(caret)
library(e1071)
library(gridExtra)
library(mgcv)
library(pROC)
library(reshape2)
library(tidyr)
library(kableExtra)
library(splines)
```

</details>

<br>

# Exploratory Data Analysis

This chapter thoroughly explores the dataset, with a specific focus on the main variable of interest: **rental prices**. Our primary goal is to develop predictive models for rental prices of both apartments and houses.

## Data Preparation and Preliminary Analysis

Before embarking on our data analysis journey, it was essential to prepare our data, here the step by step:

-   **Data Import and Merging:** The first step was to import our two datasets, named **"df_kanton"** (information on the Swiss cantons) and **"df_homegate"** (property characteristics including prices, size, number of rooms, etc). In order to work with one dataset, we merged the two datasets based on the **"Canton"** column into a new data frame.

-   **Data Filtering:** Some data points were not suitable for our intended use case because our focus is on houses and apartments. Other irrelevant property types were filtered out. Furthermore, we determined that analyzing the entire Swiss market was irrelevant for our purposes. Therefore, we limited our analysis to **Central Switzerland (Aargau (AG), Luzern (LU), Zurich (ZH)** and **Zug (ZG))** to maintain relevance and homogeneity.

-   **Preliminary Data Cleaning:** Before of cleaning the whole dataset we performed visualizations to identify and focus on interesting features.

-   **Data cleaning and preparation:**

    -   **Missing value handling:** Removed rows with missing values in key columns.

    -   **Feature engineering:** Created a new column **"Days_Difference"** to capture the time difference between two date columns.

    -   **One-Hot Encoding:** Applied to categorical variables for better compatibility with machine learning models.

    -   **Label encoding:** Converted categorical variables to numerical representations.

In summary, these steps prepared the data for further analysis and modelling by cleaning it, handling missing values, and encoding categorical variables, which are essential steps in data pre-processing for machine learning tasks.

The dataset for our Exploratory Data Analysis includes the following columns.

<details>

<summary>*Click to see all column names*</summary>

```{r EDA dataset column names, echo=FALSE, warning = FALSE, class.source = "fold-show"}
df_total <- read_excel("data/data_cleaned/data_total.xlsx")
colnames(df_total)
```

</details>

Finally, the dataset for our Modeling includes the following columns.

<details>

<summary>*Click to see all column names*</summary>

```{r Modeling dataset column names, echo=FALSE, warning = FALSE, class.source = "fold-show"}
enc_data <- read_excel("data/data_cleaned/data_total_model.xlsx")
colnames(enc_data)
```

</details>

<br>

## Data Visualization

### Properties per Canton

This bar chart shows the distribution of properties across different cantons in Switzerland. Zurich (ZH) has the highest number of properties listed, followed by Vaud (VD) and Aargau (AG), highlighting regional differences in property availability.

```{r Properties per Canton, echo=FALSE, warning = FALSE}
# Counting properties per Canton and arranging them from highest to lowest
properties_per_canton <- df_total %>%
  group_by(Canton) %>%
  summarise(Count = n()) %>%
  arrange(desc(Count)) # Arrange in descending order of count

# Define colors for specific cantons
highlight_cantons <- c("ZG", "ZH", "AG", "LU")
properties_per_canton$color <- ifelse(properties_per_canton$Canton %in% highlight_cantons, "#9C8AE6", "skyblue")

# Creating the vertical bar chart with colored bars for specific cantons
ggplot(properties_per_canton, aes(x = reorder(Canton, -Count), y = Count, fill = color)) +
  geom_bar(stat = "identity", color = "black") +
  geom_text(aes(label = Count), vjust = -0.5, color = "black", size = 2.2) + # Data labels above bars
  scale_fill_identity() + # Use the colors defined in the data frame
  labs(title = "Properties per Canton", x = "Canton", y = "Count of Properties") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), # Rotate x-axis labels for better readability
        plot.title = element_text(hjust = 0.5))
```

### Average Rental Price per Canton

The visualizations compare average rental prices across Swiss cantons, with the first chart showing the top ten cantons and highlighting the exceptionally high average rental price in Ticino compared to others.

The second chart focuses on our study cantons of Zurich, Lucerne, Aargau and Zug, illustrating more typical, affordable rental prices below CHF 2,000.

These charts highlight the significant regional differences in rental costs within Switzerland, which is useful for market analysis, investment decisions and policy formulation.

```{r, echo=FALSE, warning=FALSE}
# Function to format numbers as thousands (K)
format_k <- function(x) {
  paste0(round(x / 1000, 1), "K")
}

# Order of top 10 cantons based on average rental price
top_cantons <- df_total %>%
  group_by(Canton) %>%
  summarise(Average_Price_Gross = mean(Price_Gross, na.rm = TRUE)) %>%
  arrange(desc(Average_Price_Gross)) %>%
  slice(1:10) %>%
  .$Canton

df_total_top_10 <- df_total %>%
  filter(Canton %in% top_cantons) %>%
  mutate(Canton = factor(Canton, levels = top_cantons))

# Plot top 10 cantons
plot_top_10 <- ggplot(df_total_top_10, aes(x = Canton, y = Price_Gross)) +
  stat_summary(fun = "mean", geom = "bar", fill = "skyblue", color = "black", width = 0.8) +
  geom_text(stat = "summary", fun = mean, aes(label = format_k(..y..)), vjust = -0.3, size = 2.5) +
  labs(title = "Average Rental Price | Top 10", x = "", y = "Average Price (CHF)") +
  scale_y_continuous(labels = format_k) + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8),
        axis.title.y = element_text(size = 9),
        plot.title = element_text(size = 11))

highlight_cantons <- c("ZG", "ZH", "AG", "LU")

# Filter data for highlighted cantons 
highlighted_data <- df_total %>%
  filter(Canton %in% highlight_cantons) %>%
  group_by(Canton) %>%
  summarise(Average_Price_Gross = mean(Price_Gross, na.rm = TRUE)) %>%
  arrange(desc(Average_Price_Gross)) 

highlighted_data$Canton <- factor(highlighted_data$Canton, levels = highlighted_data$Canton)

# Plot use case cantons
plot_highlighted <- ggplot(highlighted_data, aes(x = Canton, y = Average_Price_Gross)) +
  geom_bar(stat = "identity", fill = "#9C8AE6", color = "black", width = 0.8) +
  geom_text(aes(label = format_k(Average_Price_Gross)), vjust = -0.3, size = 2.5) +
  labs(title = "Average Rental Price | Use Case", x = "", y = "Average Price (CHF)") +
  scale_y_continuous(labels = format_k) + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8),
        axis.title.y = element_text(size = 9),
        plot.title = element_text(size = 11))

# Arrange both plots side by side with specific widths
grid.arrange(plot_top_10, plot_highlighted, ncol = 2, widths = c(3, 2))
```

### Differences in rental prices between different customer segment

The box plot compare the gross rental prices between private and professional customer segments. Prices for private customers are generally lower and less varied than those for professionals, indicating a potential market segmentation by rental price.

```{r Differences in rental prices between different customer segment, echo=FALSE, warning=FALSE}
ggplot(df_total, aes(x = Customer_Segment, y = Price_Gross, fill = Customer_Segment)) +
  geom_boxplot(outlier.size = 1.5, alpha = 0.7) + 
  scale_fill_manual(values = c("Private" = "skyblue", "Professional" = "#9C8AE6")) + 
  ylim(750, 4500) +
  #coord_cartesian(ylim = c(0, quantile(df_total$Price_Gross, 0.95, na.rm = TRUE))) + 
  labs(title = "Rental Prices by Customer Segment", x = "Customer Segment", y = "Gross Price (CHF)") +
  theme_minimal() +
  theme(legend.position = "none", 
        axis.title.x = element_text(size = 9),
        axis.title.y = element_text(size = 9),
        axis.text.x = element_text(size = 8),
        axis.text.y = element_text(size =8))
```

### Influence of Property Size on Rental Price

-   **Correlation Between Property Size and Rental Price**

The scatter plot illustrates the correlation between property size and rental price across Swiss cantons. The linear trend line shows a positive correlation, indicating that as property size increases, the gross rental price also tends to increase.

This trend is consistent across both the highlighted and other cantons, suggesting that property size is a significant factor in determining rental prices across the regions analyzed. The distinction between the canton groups helps to visually assess whether there are any noticeable differences in price trends based on location, which appear to be relatively uniform across the board.

-   **Distribution of Property Size**

This histogram highlights that the majority of properties fall within smaller size brackets, with a steep drop-off as property size increases. This distribution suggests that smaller properties are more common in the market.

```{r Property Size and Rental Price, echo=FALSE, warning = FALSE}
# Define a new factor variable for canton groups
df_filtered <- df_total %>%
  mutate(canton_group = ifelse(Canton %in% c("ZH", "AG", "LU", "ZG"), 
                               "ZH, AG, LU & ZG", 
                               "Rest of Cantons"))

# Scatter Plot
plot_scatter_size <- ggplot(df_filtered, aes(x = Size_m2, y = Price_Gross, color = canton_group)) +
  geom_point(size = 1, alpha = 0.8) +
  scale_color_manual(values = c("ZH, AG, LU & ZG" = "#9C8AE6", 
                                "Rest of Cantons" = "grey")) +
  geom_smooth(method = "lm", aes(group = 1), color = "black", size = 0.5, se = FALSE) +
  labs(title = "Correlation Between Property Size and Rental Price",
       x = "Size (m²)", y = "Gross Price (CHF)") +
  theme_minimal() +
  coord_cartesian(xlim = c(0, 250), ylim = c(0, 8000)) + 
  theme(legend.position = c(0.8, 0.9), 
        legend.background = element_rect(fill = "white", colour = "black", size = 0.2), 
        legend.title = element_blank(),
        legend.margin = margin(t = 5, r = 20, b = 5, l = 10),
        plot.title = element_text(size = 11),
        axis.title.x = element_text(size = 9),
        axis.title.y = element_text(size = 9),
        axis.text.x = element_text(size = 8),
        axis.text.y = element_text(size =8))

# Histogram for Size_m2 with matched theme and limits
plot_histogram_size <- ggplot(df_total, aes(x = Size_m2)) +
  geom_histogram(bins = 30, fill = "#9C8AE6", color = "black") +
  labs(title = "Distribution of Property Size", x = "Size (m²)", y = "Frequency") +
  coord_cartesian(xlim = c(0, 350)) +  
  theme_minimal() +
  theme(legend.position = "none",
        plot.title = element_text(size = 11),
        axis.title.x = element_text(size = 9),
        axis.title.y = element_text(size = 9),
        axis.text.x = element_text(size = 8),
        axis.text.y = element_text(size = 8))

# Arrange both plots side by side with specific widths and padding
grid.arrange(plot_scatter_size, plot_histogram_size, ncol = 2, widths = c(3, 2), padding = unit(1, "lines"))
```

### Influence of Number of Rooms on Rental Price

-   **Effect of Number of Rooms on Rental Price**

This scatter plot explores how the number of rooms affects the gross rental price. While there is a general trend of increasing rental prices with more rooms, the variability in price also increases, as indicated by the spread of data points.

-   **Distribution of Number of Rooms**

The bar chart illustrates the frequency of properties based on the number of rooms. Properties with 3 to 4 rooms are the most common, which aligns with the predominance of smaller property sizes seen in the previous chart.

```{r Number of Rooms, echo=FALSE, warning = FALSE}
# Define filtering and factor setting for number of rooms related to rental price
library(dplyr)
df_filtered_rooms <- df_total %>%
  filter(Price_Gross <= 400000) %>%
  mutate(Nr_rooms = as.factor(Nr_rooms)) 

# Scatter Plot: Effect of Number of Rooms on Rental Price
plot_effect_rooms <- ggplot(df_filtered_rooms, aes(x = Nr_rooms, y = Price_Gross)) +
  geom_point(alpha = 0.6, size = 1, color = "#9C8AE6") +
  labs(title = "Effect of Number of Rooms on Rental Price", x = "Number of Rooms", y = "Gross Price (CHF)") +
  theme_minimal() +
  scale_y_log10() + # Apply logarithmic scale to y-axis for better visualization of data spread
  theme(legend.position = "none",
        plot.title = element_text(size = 11),
        axis.title.x = element_text(size = 9),
        axis.title.y = element_text(size = 9),
        axis.text.x = element_text(size = 8),
        axis.text.y = element_text(size = 8))

# Histogram: Distribution of Number of Rooms
plot_distribution_rooms <- ggplot(df_total, aes(x = Nr_rooms)) +
  geom_histogram(bins = 30, fill = "#9C8AE6", color = "black") +
  labs(title = "Distribution of Number of Rooms", x = "Number of Rooms", y = "Frequency") +
  theme_minimal() +
  theme(legend.position = "none",
        plot.title = element_text(size = 11),
        axis.title.x = element_text(size = 9),
        axis.title.y = element_text(size = 9),
        axis.text.x = element_text(size = 8),
        axis.text.y = element_text(size = 8))

# Arrange plots side by side
grid.arrange(plot_effect_rooms, plot_distribution_rooms, ncol = 2, widths = c(3, 2), padding = unit(2, "lines"))

```

<br>

# Data Modeling

*Complete*

<br>

# Linear Model

```{r Linear Model, echo=FALSE, warning=FALSE}
# Linear Model
# Target Variable: Price_Gross
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Required Packages
library(readxl)
library(ggplot2)
#---------------------------------------------------------------------------
set.seed(71)

#Splitting the data into training and testing
split_ratio <- 0.8
training_indices <- sample(1:nrow(enc_data), 
                           size=nrow(enc_data) * split_ratio,
                           replace=FALSE)

train_set <- enc_data[training_indices,]
test_set <- enc_data[-training_indices,]

#Fit the model    data[, !(names(data) %in% column_to_exclude)]
all_lm_model <- lm(Price_Gross ~ ., data=train_set)

# Retraining with only relevant variables
rel_data <- enc_data[, !(names(enc_data) %in% 
                           c("Package_Product_num",
                             "Type_num",
                             "GDP_2020_21","GDP_per",
                             "Population",
                              "Area_km2", "Density")), drop = TRUE]

rel_train_set <- rel_data[training_indices,]
rel_test_set <- rel_data[-training_indices,]

# Fit the model
few_lm_model <- lm(Price_Gross ~ ., data=rel_train_set)

# Retraining with fewer variables
simp_data <- enc_data[, c("Nr_rooms", "Category_num", "Price_Gross")]

simp_train_set <- simp_data[training_indices,]
simp_test_set <- simp_data[-training_indices,]

# Fit the model
simple_lm_model <- lm(Price_Gross ~ ., data=simp_train_set)

#Model Predictions
all_lm_predictions <- predict(all_lm_model, newdata=test_set)
few_lm_predictions <- predict(few_lm_model, 
                              newdata=rel_test_set)
simple_lm_predictions <- predict(simple_lm_model, 
                                 newdata=simp_test_set)

#Model Evaluation

rmse <- sqrt(mean((all_lm_predictions - test_set$Price_Gross)^2))
mae <- mean(abs(all_lm_predictions - test_set$Price_Gross))

new_rmse <- sqrt(mean((few_lm_predictions - rel_test_set$Price_Gross)^2))
new_mae <- mean(abs(few_lm_predictions - rel_test_set$Price_Gross))

simp_rmse <- sqrt(mean((simple_lm_predictions - simp_test_set$Price_Gross)^2))
simp_mae <- mean(abs(simple_lm_predictions - simp_test_set$Price_Gross))

```

## Purpose and Target

We decided to employ a linear regression model in order to predict prices for different rental properties, specifically apartments. We created three different models, each taking into account different features to help us gain better insight.

### Linear Model 1

The first model (*all_lm_model*) takes all available property features into account to train and make predictions. From this we were able to see that some of those we included were not relevant to the target variable, as seen below:

<details>

<summary>*Click to see the summary of the first linear model*</summary>

```{r First Linear model, echo=FALSE, warning = FALSE, class.source = "fold-show"}
summary(all_lm_model)
```

</details>

*As we can see in the model summary, some variables are converted into factors and interpreted separately, we can also see 'Nr_rooms' is relevant in general, but Package_Product and Category, for example, are not.*

With this information we were able to refine our selection of variables for training the following models, one with only a few variables, and one with only one.

### Linear Model 2

The next model ( *few_lm_model* ) we trained with fewer dependent variables, only those of high importance. For this one, we took into account only the ones that were calculated to be significant to the model's performance.

<details>

<summary>*Click to see the summary of the second linear model*</summary>

```{r Few Model Summary, echo=FALSE, warning = FALSE, class.source = "fold-show"}
summary(few_lm_model)
```

</details>

*Here we can see that the variables taken into account were Canton, Days_Difference, Category, Customer_Segment, Nr_rooms and Size_m2.*

### Linear Model 3

The last model generated took into account only *two* variable as a predictor: *Nr_rooms* and *Category_num2*. The number of rooms seems to have one of, if not the biggest effect on rental price of a property, which is why we went with this variable for analysis. The second customer category also has a significant effect, so it was included.

<details>

<summary>*Click to see the summary of the third linear model*</summary>

```{r Simple Model Summary, echo=FALSE, warning = FALSE, class.source = "fold-show"}
summary(simple_lm_model)$coef
```

</details>

## Interpretation

All three models offer us some insight into the behavior of rental prices for different properties. Interestingly, our research thus far shows that the Canton of said property is not that influential in its rental price, except for Canton 3.

*(Canton 1: Argau, Canton 2: Luzern, Canton 3: Zürich, Canton 4: Zug)*

For desired results, we will focus on the second model, which includes all the variables considered significant for the best results.

```{r LM Price Vs Category, echo=FALSE}
lm_plot1 <- data.frame(rel_data$Nr_rooms, rel_data$Price_Gross)
lm_plot2 <- data.frame(rel_data$Category_num, rel_data$Price_Gross)

colors <- ifelse(rel_data$Category_num<1.5, "#9C8AE6", "skyblue")

plot(rel_data$Category_num, rel_data$Price_Gross, xlab="", ylab="Rental Price of Property", main="Price of a Rental Property as a Result of the Customer Category", col=colors, pch=19, xlim=c(0.5,2.5), xaxt="n")
axis(1, at=c(1,2), labels = c("Category 1", "Category 2"))
```

In this diagram we can observe that, though there are fewer instances of Category 2 (House) compared to Category 1 (Apartment), Category 2 properties tend to have higher rental prices. This does not, however, indicate that they may be the more lucrative option, as houses tend to incur other costs that are either not present, or the cost is split in the community, in the case of apartments.

```{r LM First Model Plot, echo=FALSE}

plot(lm_plot1, xlab="Number of Rooms", ylab="Rental Price of Property", main="Price of a Rental Property as a result of the Number of Rooms", col="#9C8AE6", xlim=c(0,8), ylim=c(0,10000),
     cex=0.8)

```

In the illustration above, we depict the relationship between the number of rooms in a property and the gross price of the same. We can observe an upward trend, which leads to believe that a higher number of rooms leads to a higher price, which would make sense. This information must also be taken into consideration together with other variables, like the property's area in meters squared, average size of rooms, location, etc.

```{r 3LM Plots, echo=FALSE}
# Plot
plot(rel_data$Nr_rooms, rel_data$Price_Gross, pch=19, main="Multiple Linear Models", xlab="Number of Rooms", ylab="Rental Price of Property", cex=0.8, xlim=c(0,8), ylim=c(0,10000),
     col="#9C8AE6")

# Lines
abline(all_lm_model, col=rgb(156/255, 138/255, 230/255, 0.7), lwd=2, lty=1)
abline(few_lm_model, col=rgb(0/255, 0/255, 0/255), lwd=2, lty=2)
abline(simple_lm_model, col=rgb(135/255, 206/255, 250/255, 0.7), lwd=2, lty=1)

# Legend
legend("topright", legend=c("All Variables", "Relevant Variables", "One Variable"),
       col=c(rgb(156/255, 138/255, 230/255, 0.7),
             rgb(0/255, 0/255, 0/255),
             rgb(135/255, 206/255, 250/255, 0.7)),
       lwd=2, lty=c(1, 2, 1), cex=0.8)

```

*Looking closely, we can see that the model with all variables, and the one with relevant variables, have almost the exact same performance*

In the visual aid above, we can observe how each of these three models compare to each other and the data. The model that takes into account only the number of rooms as an independent variable is more generous when predicting prices of properties. This does not mean, however, that it is a better or worse model.

## Model Comparison

Next, we will take a look at how these models make predictions, and compare them. Two of our models have been trained on practically the same data, as seen in the previous visuals, where we outlined that the first model is trained on ALL available variables in the data, and the second model is trained only on the relevant variables. So far it seems both of them will lead to very similar predictions, so we decided to test that theory below.

```{r Model Evaluation Table, echo=FALSE}
# Calculate predictions for each model
all_lm_predictions <- predict(all_lm_model, newdata = test_set)
few_lm_predictions <- predict(few_lm_model, newdata = test_set)
simple_lm_predictions <- predict(simple_lm_model, newdata = test_set)

# Calculate eval metrics for each model
rmse_all <- sqrt(mean((all_lm_predictions - test_set$Price_Gross)^2))
mae_all <- mean(abs(all_lm_predictions - test_set$Price_Gross))
r_squared_all <- summary(all_lm_model)$r.squared
adj_r_squared_all <- summary(all_lm_model)$adj.r.squared
aic_all <- AIC(all_lm_model)
bic_all <- BIC(all_lm_model)

rmse_few <- sqrt(mean((few_lm_predictions - test_set$Price_Gross)^2))
mae_few <- mean(abs(few_lm_predictions - test_set$Price_Gross))
r_squared_few <- summary(few_lm_model)$r.squared
adj_r_squared_few <- summary(few_lm_model)$adj.r.squared
aic_few <- AIC(few_lm_model)
bic_few <- BIC(few_lm_model)

rmse_simple <- sqrt(mean((simple_lm_predictions - test_set$Price_Gross)^2))
mae_simple <- mean(abs(simple_lm_predictions - test_set$Price_Gross))
r_squared_simple <- summary(simple_lm_model)$r.squared
adj_r_squared_simple <- summary(simple_lm_model)$adj.r.squared
aic_simple <- AIC(simple_lm_model)
bic_simple <- BIC(simple_lm_model)

# Create a dataframe with metrics
eval_metrics <- data.frame(
  Model = c("All Variables", "Relevant Variables", "One Variable"),
  RMSE = c(rmse_all, rmse_few, rmse_simple),
  MAE = c(mae_all, mae_few, mae_simple),
  R_Squared = c(r_squared_all, r_squared_few, r_squared_simple),
  Adj_R_Squared = c(adj_r_squared_all, adj_r_squared_few, adj_r_squared_simple),
  AIC = c(aic_all, aic_few, aic_simple),
  BIC = c(bic_all, bic_few, bic_simple)
)

# Display the table
# Create the table with styling
kbl(eval_metrics, align = "l") %>%
  kable_classic(full_width = F, html_font = "Segoe UI") %>%
  kable_styling(latex_options = c("striped", "hold_position"), position = "center", font_size = 10) %>%
  column_spec(1, bold = TRUE, width = "3cm") %>%
  column_spec(2:7, width = "2cm") %>%
  row_spec(0, background = "#9C8AE6", color = "white", bold = TRUE)

```

As we can see from the models' two error metrics (*MSE and RMSE*), the models trained with more variables have a slightly better performance on the test data. The R-Squared and Adjusted R-Squared values indicate how well the model can explain variance in the data, and the two initial models have performed better in this sense as well. As far as AIC and BIC, we can see that though there is not much difference, the model trained only on relevant variables slightly outperforms the other two, indicating a better trade-off between model fit and simplicity.

## Conclusion

Our linear models suggest that properties, specifically houses, with more rooms go for a higher price. This doesn't necessarily indicate that they may be the best choice for renting out to customers, but it does suggest that they are the properties which generate the highest gross payment on a regular basis.

It is also made clear in our findings, that although the number of rooms is a good indicator as to how much a property may be worth renting out for, it is not necessarily the best way to go about this. Previously, we saw that the model trained only on the number of rooms in a property predicted higher rental prices than the other two. We cannot yet accept that conclusion, as the only confident insight we have been able to find is that a linear model is not the best fit for a problem that has so many significant variables, as shown previously in the two initial linear model summaries.

<br>

# GLM - Generalised Linear - Binomial

```{r GLM Binomial, echo=FALSE, warning=FALSE}
# Read data
enc_data <- read_excel("data/data_cleaned/data_total_model.xlsx")
enc_data <- enc_data %>%
  mutate(fast.sale = ifelse(Days_Difference <= 17, 1, 0))

# Split the data into training and testing sets
set.seed(71)
split_ratio <- 0.8
training_indices <- sample(1:nrow(enc_data), size = nrow(enc_data) * split_ratio, replace = FALSE)
train_set <- enc_data[training_indices, ]
test_set <- enc_data[-training_indices, ]

# Fit a binomial logistic regression model to predict fast sale
binomial.model <- glm(fast.sale ~ Price_Gross + Nr_rooms + Size_m2 + Canton_num + Customer_Segment_num + Category_num, family = binomial, data = train_set)

# Calculate and print odds ratios from the model coefficients
coef <- coef(binomial.model)
odds_ratios <- exp(coef)

# Get and display fitted values (predicted probabilities) for the training set
fitted_values_train <- fitted(binomial.model)
train_set$fitted_values <- fitted_values_train

# Predict probabilities for the test set and display the first few predictions
predicted_probabilities_test <- predict(binomial.model, test_set, type = "response")
test_set$fitted_values <- predicted_probabilities_test

# Convert predicted probabilities to binary outcomes using a 0.5 threshold
predicted_classes <- ifelse(predicted_probabilities_test > 0.5, 1, 0)
actual_classes <- test_set$fast.sale

# Create and print the confusion matrix
conf_matrix <- table(Predicted = predicted_classes, Actual = actual_classes)
conf_matrix_percent <- prop.table(conf_matrix) * 100

# Calculate precision, recall, and accuracy and print them
precision <- sum(predicted_classes == actual_classes & actual_classes == 1) / sum(predicted_classes == 1)
recall <- sum(predicted_classes == actual_classes & actual_classes == 1) / sum(actual_classes == 1)
accuracy <- sum(predicted_classes == actual_classes) / length(actual_classes)

# Fit a quasi-Poisson regression model to predict the number of rooms (Nr_rooms)
modelo_quasi_poisson <- glm(Nr_rooms ~ Price_Gross + Size_m2, family = quasipoisson, data = train_set)

# Predict values for the test set and calculate mean squared error (MSE)
predicted_values_test_quasi <- predict(modelo_quasi_poisson, newdata = test_set, type = "response")
test_set$fitted_values_quasi <- predicted_values_test_quasi

mse_test_quasi <- mean((test_set$Nr_rooms - test_set$fitted_values_quasi)^2)

# Calculate and print root mean squared error (RMSE)
rmse_test_quasi <- sqrt(mse_test_quasi)
```

## Purpose and Target

Quick sales or rentals are crucial for Zug Estates in the commercial real estate world. Fast transactions minimize vacancy periods, ensuring steady cash flow and maximizing returns on investment. This efficiency not only benefits property owners by reducing holding costs but also attracts potential clients who value quick and smooth processes.

We decided to use a Generalized Linear Model, Binomial to modelize it.

## Interpretation

Before jumping into the model, we needed to do some adjustments to the data:

<details>

<summary>*Click to see the results*</summary>

```{r GLM Binomial 1, echo=FALSE, warning = FALSE, class.source = "fold-show"}
mean(enc_data$Days_Difference)  
```

</details>

The average time it takes an owner to rent out a flat or house is 16 days. Therefore, we have taken the average as a reference, if a property takes less than 17 days it is categorised as 'Fast sale' and coded as 1, otherwise it is coded as 0.

Then we decided to divide the data set (enc_data) into two subsets: one for training (train_set) and another for testing (test_set). The training set is used to tune the model, while the test set is used to evaluate the performance of the model. We will use it later.

In summary, the logistic regression model indicates that Price_Gross, certain Canton categories, and Customer_Segment have significant effects on the likelihood of a quick sale. Specifically, higher prices decrease the probability, while certain regions (Canton_num3 and Canton_num4) and customer segments have varying influences. The fit of the model is reasonable, as indicated by the reduction in deviance and the AIC value. Mas concreatmente, podemos extraer informacion muy interesante que relfexionaremos tras analizar mas en profunidad el modelo.

-   **Price_Gross:** The negative coefficient indicates that higher gross prices are associated with lower probabilities of a quick sale. This suggests that properties priced higher may take longer to sell, likely due to a smaller pool of potential buyers or perceived overvaluation. However the coefieicnete is so small que no se puede desestimar. Sorpresivamente para Zug Estates el precio no parece jugar un papel significativo.

-   **Nr_rooms:** Not significant (p-value = 0.296563), which implies that the number of rooms may not be a decisive factor for quick sales in Central Switzerland.

-   **Size_m2:** Not significant (p-value = 0.703451), thus, the size of the property alone does not strongly influence quick sales.

-   **Canton_num:**

    -   **Canton_num2:** Negative effect, not significant (p-value = 0.07185)

    -   **Canton_num3:** Positive effect, significant (p-value = 0.006502)

    -   **Canton_num4:** Positive effect, significant (p-value = 0.000767)

Properties in Canton_num3 and Canton_num4 have significantly higher odds of quick sales, suggesting these areas are in high demand or have market conditions conducive to faster transactions. This two canton are equivalent to X and Y which mean that the probabilities of a quick sale in Canton_num3 are approximately 17.46 times greater than in the reference canton and for Canton_num4 are approximately 2.27 times higher than in the baseline canton.

<details>

<summary>*Click to see the results*</summary>

```{r GLM Binomial 2, echo=FALSE, warning = FALSE, class.source = "fold-show"}
coef <- coef(binomial.model)
odds_ratios <- exp(coef)
print(odds_ratios)  
```

</details>

This is important for the Zug-based company's future property operations.

Later we have added to the original dataset a new column named fitted_values to the training set (train_set). This column contains the predicted probabilities obtained from the fitted model. The fitted values are the predicted probabilities that the dependent variable (fast.sale) equals 1 (i.e., a quick sale) for each observation in the training set.

<details>

<summary>*Click to see the results*</summary>

```{r GLM Binomial 3, echo=FALSE, warning = FALSE, class.source = "fold-show"}
fitted_values_train <- fitted(binomial.model) 
train_set$fitted_values <- fitted_values_train 
head(train_set[, c("fast.sale", "fitted_values")]) 
```

</details>

Finally, we have made Predictions on the Test Set. The interpretation is the interpretation of the actual outcome , if it was a quick sale (1) or not (0). For example, in the first case, the model predicted a 78.5% probability of a quick sale. This prediction is quite accurate and confident.

<details>

<summary>*Click to see the results*</summary>

```{r GLM Binomial 4, echo=FALSE, warning = FALSE, class.source = "fold-show"}
predicted_probabilities_test <- predict(binomial.model, test_set, type = "response") 
test_set$fitted_values <- predicted_probabilities_test 
head(test_set[, c("fast.sale", "fitted_values")]) 
```

</details>

To measure the quality of the model so that Zug Estates can make adequate predictions:

<details>

<summary>*Click to see the results*</summary>

```{r GLM Binomial 5, echo=FALSE, warning = FALSE, class.source = "fold-show"}
predicted_classes <- ifelse(predicted_probabilities_test > 0.5, 1, 0) 
actual_classes <- test_set$fast.sale 
conf_matrix <- table(Predicted = predicted_classes, Actual = actual_classes) 
conf_matrix_percent <- prop.table(conf_matrix) * 100 
print(round(conf_matrix_percent, 2)) 
print(conf_matrix) 
```

</details>

## Conclusion

Without being exhaustive, here are some conclusions about the model:

-   **False Negatives (FN):** The model incorrectly predicted 15 instances as not quick sales (0) when they were quick sales (1), which is 7.77% of the total predictions. This indicates missed opportunities where the model failed to identify actual quick sales.

-   **True Positives (TP):** The model correctly predicted 129 instances as quick sales (1), which is 66.84% of the total predictions. This shows a strong performance in correctly identifying quick sales.

Finally, with respect to performance metrics that are standard in Machine Learning:

<details>

<summary>*Click to see the results*</summary>

```{r GLM Binomial 6, echo=FALSE, warning = FALSE, class.source = "fold-show"}
precision <- sum(predicted_classes == actual_classes & actual_classes == 1) / sum(predicted_classes == 1) 
recall <- sum(predicted_classes == actual_classes & actual_classes == 1) / sum(actual_classes == 1) 
accuracy <- sum(predicted_classes == actual_classes) / length(actual_classes) 
print(paste("Precision:", precision)) 
print(paste("Recall:", recall)) 
print(paste("Accuracy:", accuracy)) 
```

</details>

-   **Precision** indicates how reliable the model is when it predicts a quick sale. With a precision of 77.2%, the model has a moderate false positive rate, meaning some of the predicted quick sales are actually not quick sales.

-   **Recall** measures how well the model identifies actual quick sales. With a high recall of 89.6%, the model successfully identifies most of the quick sales, indicating a low false negative rate.

-   **Accuracy** provides an overall assessment of model performance. With an accuracy of 72.5%, the model correctly predicts the outcome for a majority of the cases but still makes errors in a significant number of cases.

<br>

# GLM - Generalised Linear - Poisson

## Purpose and Target

Following the line of what was written on these lines, we want to provide Zug estates with a tool that allows predicting how many rooms would need a house to fit a market. For this we will use Quasi Poisson model, which is a variant of the Poisson regression model that is used to handle overdispersion in the data. Instead of assuming that the variance is equal to the mean (as in Poisson), the quasi-Poisson model allows the variance to be a function of the mean, but more flexible.

For Zug Estates, understanding what properties are in demand is key. By predicting the number of rooms accurately, they can sell or rent properties faster. This reduces vacancy periods, keeps cash flow steady, and boosts investment returns. It also lowers holding costs and speeds up transactions, making property owners and clients happy with quick, smooth processes.

## Interpretation

We decided to perform a logarithmic transformation on specific columns within two data sets: train_set and test_set. By applying these transformations, our code is preparing the train_set and test_set for better performance in subsequent glm model.

<details>

<summary>*Click to see the code*</summary>

```{r GLM Poisson 1, echo=FALSE, class.source = "fold-show"}
train_set$log_Price_Gross <- log(train_set$Price_Gross + 1) 
train_set$log_Size_m2 <- log(train_set$Size_m2 + 1) 
test_set$log_Price_Gross <- log(test_set$Price_Gross + 1) 
test_set$log_Size_m2 <- log(test_set$Size_m2 + 1) 
```

</details>

Here the **model**:

<details>

<summary>*Click to see the code*</summary>

```{r GLM Poisson 2, echo=FALSE, class.source = "fold-show"}
quasipoisson.model <- glm(Nr_rooms ~ log_Price_Gross + log_Size_m2 + Category_num + Canton_num, family = quasipoisson, data = train_set) 
```

</details>

The model takes into account a previous logarithmic transformation that we have done so it must be interpreted in those terms, since it must fit what we want to put into it. The relevance of the price and the square meters in addition to one of the cantons to calculate the number of rooms is attested by the p value having statistical significance. However, whether or not the apartment is being rented by a professional or several of the cantons is not relevant, since its value is greater than 0.05.

When interpreting the price, for example, we can see that a 1% increase in Size_m2 increases the expected count of rooms by 𝑒0.00593205≈1.00593, or approximately 0.593%.

-   Each point on the scatter plot represents a pair of actual and predicted values for a particular observation in the test set.

-   The red line is the line of perfect prediction, where the predicted values would exactly equal the actual values (i.e., [Ecuación]y=x). Ideally, all points would lie on this line if the model predictions were perfect.

```{r GLM Poisson 3, echo=FALSE, warning=FALSE}
predicted_values_test_quasi_log <- predict(quasipoisson.model, newdata = test_set, type = "response") 
test_set$fitted_values_quasi_log <- predicted_values_test_quasi_log 

ggplot(test_set, aes(x = Nr_rooms, y = fitted_values_quasi_log)) +
  geom_point(alpha = 0.7, color = "#9C8AE6") +  
  geom_abline(intercept = 0, slope = 1, color = "black") +  
  labs(title = "Actual vs Predicted Values for Nr. of Rooms (Log Transformed Quasi-Poisson)",
       x = "Actual Nr. of Rooms",
       y = "Predicted Nr. of Rooms") +
  theme_minimal() +
  coord_cartesian(ylim = c(0, 8)) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 11, face = "bold"),
    axis.title = element_text(size = 10)
  )
```

This scatter plot demonstrates that the Quasi-Poisson model with log-transformed predictors effectively captures the overall trend. As the actual number of rooms increases, the predicted values also rise correspondingly, indicating that the model successfully identifies the general relationship between the predictors and the target variable.

The trend highlights the model's capability to generalize well across the range of data points, reflecting a solid understanding of the underlying patterns in the dataset.

Despite some deviations from the line of perfect prediction (in red,), the model shows a promising performance. The increasing dispersion of points with higher actual values suggests that while the model is less precise for larger counts of rooms, it still maintains a coherent predictive behavior.

<br>

# GAM - Generalised Additive Model

```{r GAM code 0, echo=FALSE, warning=FALSE}
d.properties <- read_excel("data/data_cleaned/data_total_model.xlsx")

```

## Purpose and Target

### Purpose

The purpose is to analyze and model the relationship between property size (in square meters) and price using various statistical techniques. The goal is to determine the best-fitting model that accurately predicts property prices based on the available data. By fitting and comparing different models Generalized Additive Models (GAMs), we aim to identify the most reliable and insightful model. Additionally, the report provides recommendations for further improving the model and making more accurate predictions.

-   **Data Analysis:** Understand the dataset and the key variables involved.

-   **Model Fitting:** Apply different statistical models to the data.

-   **Model Comparison:** Evaluate and compare the performance of the models.

-   **Recommendations:** Provide suggestions for future improvements and next steps.

## Interpretation

### Quadratic Model Plot

The graph shows the relationship between Size_m2 and Price_Gross using a quadratic model. The black line represents the fitted values from the quadratic model, and the shaded area represents the confidence interval around the fitted values.

**Key Observations**

-   **Non-linear Relationship** between Size_m2 and Price_Gross. Initially, Price_Gross increases with Size_m2 up to about 300-400 m², after which it starts to decrease.

-   **The Confidence Intervals** widen significantly beyond 300 m², indicating increased uncertainty in the predictions for larger properties. This suggests that the model is less reliable for larger properties due to fewer data points in this range.

-   **Data Distribution.** Most of the data points are clustered below 300 m², with few data points for larger properties. The fit of the model is more certain where there are more data points, and less certain where data points are sparse.

```{r GAM code 1 , echo=FALSE, warning=FALSE}
gg.density.site <- ggplot(data = d.properties, mapping = aes(y = Price_Gross, x = Size_m2)) + 
  geom_point(alpha = 0.6, size = 0.8, color = "#9C8AE6") +
  labs(title = "Effect of Size on Gross Price", x = "Size (m²)", y = "Gross Price (CHF)") +  
  theme_minimal() +  
  theme(legend.position = "none",
        plot.title = element_text(size = 11),
        axis.title.x = element_text(size = 9),
        axis.title.y = element_text(size = 9),
        axis.text.x = element_text(size = 8),
        axis.text.y = element_text(size = 8)) +
  geom_smooth(color = "black")  

print(gg.density.site)

```

**Next Steps for the GAM**

Given the observations from the quadratic model, we can take the following steps to improve our model using GAM:

-   **Filter the data.** Consider filtering the data to include only properties with Size_m2 less than 300 m². This can help improve the reliability of the model in the range where we have more data.

-   **Transform variables.** Transform Size_m2 to log(Size_m2) if a log transformation better captures the relationship and stabilises the variance.

-   **Include interactions** with other significant variables such as Nr_rooms to capture more complex relationships.

-   **Use smoothing splines in GAM**, which can capture non-linear relationships more flexibly than a quadratic model.

Once these parameters are clear, we will implement them in our GAM models and see which ones perform best.

### Basic GAM

Starting with fitting a basic GAM to capture more complex non-linear relationships:

<details>

<summary>Click to see the summary</summary>

```{r GAM code 4, echo=FALSE, warning = FALSE, class.source = "fold-show"}

gam.properties <- gam(Price_Gross ~ s(Size_m2), data = d.properties)
summary(gam.properties)
```

</details>

*R-squared (adj.): 0.389, indicating that 38.9% of the variability is explained. Deviance explained: 39.5%.*

### GAM with Multiple Predictors

To account for additional factors, a GAM including multiple predictors was fitted:

<details>

<summary>Click to see the summary</summary>

```{r GAM code 5, echo=FALSE, warning = FALSE, class.source = "fold-show"}

gam.properties.full <- gam(Price_Gross ~ s(Size_m2) + Days_Difference + Nr_rooms + GDP_per + Population + Area_km2 + Density, data = d.properties)
summary(gam.properties.full)
```

</details>

*Adjusted R-squared: 0.413, indicating 41.3% of variability is explained. Significant predictors: Days_Difference, Nr_rooms, GDP_per, Population.*

### Filtered GAM

To focus on properties with Size_m2 less than 300, a filtered GAM was fitted:

<details>

<summary>Click to see the summary</summary>

```{r GAM code 6, echo=FALSE, warning = FALSE, class.source = "fold-show"}

d.properties.filtered <- d.properties %>% filter(Size_m2 < 300)
gam.properties.filtered <- gam(Price_Gross ~ s(Size_m2) + Days_Difference + Nr_rooms + GDP_per + Population + Area_km2 + Density, data = d.properties.filtered)
summary(gam.properties.filtered)
```

</details>

*Adjusted R-squared: 0.431, explaining 43.1% of variability.*

### Log-Transformed GAM

A log transformation of Size_m2 was applied to improve model fit:

<details>

<summary>Click to see the summary</summary>

```{r GAM code 7, echo=FALSE, warning = FALSE, class.source = "fold-show"}

d.properties <- d.properties %>% mutate(log_Size_m2 = log(Size_m2))
gam.properties.log <- gam(Price_Gross ~ s(log_Size_m2) + Days_Difference + Nr_rooms + GDP_per + Population + Area_km2 + Density, data = d.properties)
summary(gam.properties.log)
```

</details>

*Adjusted R-squared: 0.388, explaining 38.8% of variability.*

### GAM with Interactions

To explore interactions, a GAM including interaction terms was fitted:

<details>

<summary>Click to see the summary</summary>

```{r GAM code 8, echo=FALSE, warning = FALSE, class.source = "fold-show"}

gam.properties.interaction <- gam(Price_Gross ~ s(Size_m2) + Days_Difference + Nr_rooms + GDP_per + Population + Area_km2 + Density + s(Size_m2, by=Nr_rooms), data = d.properties)
summary(gam.properties.interaction)
```

</details>

*Adjusted R-squared: 0.449, explaining 44.9% of variability. Significant interaction: s(Size_m2):Nr_rooms.*

## Conclusion

### Model Comparison Table

As can be seen in the table and in the summaries of each model, based on the adjusted R-squared and the explained variance, the best fitting model is the GAM with interaction terms.

This model explains 44.9% of the variability in Price_Gross and includes significant interactions between **Size_m2** and **Nr_rooms**.

```{r Table GAMs Comparison, echo=FALSE, warning = FALSE}
# Load the kableExtra library
library(kableExtra)

# Create the data frame for model comparison
model_comparison <- data.frame(
  Model = c("GAM - Size_m2", "GAM - Full", "GAM - Filtered", "GAM - Log", "GAM - Interaction"),
  `R_squared_Adj` = c(0.389, 0.413, 0.431, 0.388, 0.449),
  `Deviance_Explained` = c("39.5%", "41.9%", "43.9%", "39.6%", "45.7%"),
  `GCV_Score` = c(754590, 726970, 685020, 759320, 683690)
)

# Create the table with styling
kbl(model_comparison, align = "l") %>%
  kable_classic(full_width = F, html_font = "Segoe UI") %>%
  kable_styling(latex_options = c("striped", "hold_position"), position = "center", font_size = 10) %>%
  column_spec(1, bold = TRUE, width = "3cm") %>%
  column_spec(2:4, width = "2cm") %>%
  row_spec(0, background = "#9C8AE6", color = "white", bold = TRUE)
```

### Next Steps

**1. Higher-Order Polynomial Terms:** Explore cubic or higher-order terms.

**2. Advanced GAMs:** Consider fitting more complex GAMs with different smoothing parameters.

**3. Additional Predictors:** Include more relevant predictors to improve model accuracy.

**4. Cross-Validation:** Perform cross-validation to validate model robustness.

These steps will help in further refining the model and ensuring more accurate predictions for property prices.

## Model Predictions

The analysis aims to forecast rental prices for houses and apartments. Noticing that the highest volume of rental prices falls within the range of 50 to 150 square meters, we have decided to focus our prediction on rental prices within this segment. This size range can be understood as a key market segment for Zug Estates, given its commercial relevance.

**Methodology used**

-   **Cross-validation:** A 10-fold cross-validation was used to increase the robustness of the model, involving repeated training and testing.

-   **Model fitting:** as we have seen in the Model Comparison Table, we choosed the GAM - Interaction Model which included between its predictors including property size, listing period, number of rooms, GDP per capita, population, area and population density, and an interaction between property size and number of rooms.

**Performance metrics**

-   **RMSE:** The accuracy of the model in predicting rental prices was measured by the RMSE, which averaged CHF 1005.04 across all replicates, indicating the model's predictive ability.

<br>

```{r GAM Plot, echo=FALSE, warning = FALSE}
# Train Best model: GAM with interaction | Test Data with Cross Validation
# Define the number of folds for cross-validation
set.seed(123)
folds <- createFolds(d.properties$Price_Gross, k = 10, list = TRUE)

# Initialize vectors
all_predictions <- c()
all_actuals <- c()  
rmse_values <- c()


# Perform cross-validation
for(i in 1:length(folds)) {
  # Split the data into training and testing sets
  test_indices <- folds[[i]]
  train_data <- d.properties[-test_indices, ]
  test_data <- d.properties[test_indices, ]
  
  # Fit the model on the training data
  gam_model <- gam(Price_Gross ~ s(Size_m2) + Days_Difference + Nr_rooms + GDP_per + Population + Area_km2 + Density + s(Size_m2, by = Nr_rooms), data = train_data)
  
  # Predict on the test data
  predictions <- predict(gam_model, newdata = test_data)
  
  # Store the predictions and actual values
  all_predictions <- c(all_predictions, predictions)
  all_actuals <- c(all_actuals, test_data$Price_Gross)
  
  # Calculate RMSE for the current fold
  rmse <- sqrt(mean((predictions - test_data$Price_Gross)^2))
  rmse_values <- c(rmse_values, rmse)
}

# Calculate the average RMSE across all folds
average_rmse <- mean(rmse_values)

# Print the average RMSE
#print(average_rmse)

# Create a data frame with predictions and actual values
results_df <- data.frame(Predicted = all_predictions, Actual = all_actuals)

# Create an index for each test instance
results_df <- results_df %>%
  mutate(Index = 1:nrow(results_df))

# Plot the predicted vs. actual prices as lines
line_plot <- ggplot(results_df, aes(x = Index)) +
  geom_line(aes(y = Actual, color = "Actual Price"), size = 0.5, alpha = 0.7) +
  geom_line(aes(y = Predicted, color = "Predicted Price"), size = 0.75, alpha = 0.7) +
  labs(title = "Comparison of Prices",
       subtitle = paste("Average RMSE across 10 folds:", round(average_rmse, 2)),
       x = "Size (m²)",
       y = "Rental Price (CHF)",
       color = "Legend") +
  theme_minimal() +
  coord_cartesian(xlim = c(0, 150), ylim = c(0, 8000)) +
  scale_color_manual(values = c("Actual Price" = "black", "Predicted Price" = "#9C8AE6")) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 11, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 10),
    axis.title = element_text(size = 10),
    legend.text = element_text(size = 8),
    legend.position = c(0.85, 0.9), 
    legend.background = element_rect(fill = "white", colour = "black", size = 0.2), 
    legend.title = element_blank(),
    legend.margin = margin(t = 5, r = 20, b = 5, l = 10)
  )

# Limit the data to properties with sizes up to 500 m²
limited_data <- d.properties %>%
  filter(Size_m2 <= 150)

# Initialize vectors to store predictions and actual prices for limited data
limited_predictions <- c()
limited_actuals <- c()

# Perform cross-validation for limited data
for(i in 1:length(folds)) {
  # Split the data into training and testing sets
  test_indices <- folds[[i]]
  train_data <- limited_data[-test_indices, ]
  test_data <- limited_data[test_indices, ]
  
  # Fit the model on the training data
  gam_model <- gam(Price_Gross ~ s(Size_m2) + Days_Difference + Nr_rooms + GDP_per + Population + Area_km2 + Density + s(Size_m2, by = Nr_rooms), data = train_data)
  
  # Predict on the test data
  predictions <- predict(gam_model, newdata = test_data)
  
  # Store the predictions and actual values
  limited_predictions <- c(limited_predictions, predictions)
  limited_actuals <- c(limited_actuals, test_data$Price_Gross)
}

# Create a data frame with predictions and actual values for limited data
limited_results_df <- data.frame(Predicted = limited_predictions, Actual = limited_actuals)

# Combine the predicted and actual prices into a single column for plotting
long_results_df <- limited_results_df %>%
  pivot_longer(cols = c("Predicted", "Actual"), names_to = "Type", values_to = "Price")

# Plot box plots of the predicted and actual prices
box_plot <- ggplot(long_results_df, aes(x = Type, y = Price, fill = Type)) +
  geom_boxplot(alpha = 0.7) +
  labs(title = "Distribution Prices",
       x = "Type",
       y = "Rental Price (CHF)") +
  scale_fill_manual(values = c("Actual" = "grey", "Predicted" = "#9C8AE6")) +
  theme_minimal() +
  coord_cartesian(ylim = c(0, 6000)) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 11, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 10),
    axis.title = element_text(size = 10),
    legend.position = "none"
  )

# Arrange both plots side by side with specific widths and padding
grid.arrange(line_plot, box_plot, ncol = 2, widths = c(3, 2), padding = unit(1, "lines"))
```

**Results**

-   **Price comparison:** Visual comparison showed the variability between predicted (purple) and actual (black) prices, with some variation across different property sizes.

-   **Price distribution:** Predicted prices showed a narrower interquartile range and fewer extremes compared to actual prices, suggesting under-prediction of outliers.

To conclude we can say that The GAM model is reasonably effective at predicting rental prices for smaller properties, capturing general trends despite some variation.

**Recommendations**

-   **Refine the model:** Consider additional predictors or interactions to improve accuracy.

-   **Outlier analysis:** Investigate causes of outliers to refine predictions.

-   **Focus on target segment:** Ensure data accuracy for properties under 150 square metres to align with market interest.

This foundational analysis helps Zug Estates refine rental price predictions to maintain competitiveness and market insight.

<br>

# Neural Network

```{r NeuralNet - Base, echo=FALSE}
# Reading in data
enc_data <- read_excel("data/data_cleaned/data_total_model.xlsx")
enc_data <- enc_data[, !(names(enc_data) %in% "X"), drop = TRUE]

# Feature Engineering
enc_data$Price_per_m2 <- enc_data$Price_Gross / enc_data$Size_m2
q3 <- median(enc_data$Price_per_m2) + IQR(enc_data$Price_per_m2) / 2
enc_data$High_Ticket <- enc_data$Price_per_m2 > q3
enc_data$High_Ticket <- ifelse(enc_data$High_Ticket, 1, 0)

# Declaring Categorical Variables
enc_data$Canton_num <- as.numeric(as.factor(enc_data$Canton_num))
enc_data$Customer_Segment_num <- as.numeric(as.factor(enc_data$Customer_Segment_num))
enc_data$Category_num <- as.numeric(as.factor(enc_data$Category_num))
enc_data$Nr_rooms <- as.numeric(enc_data$Nr_rooms) # Treating Nr_rooms as continuous
enc_data$Package_Product_num <- as.numeric(as.factor(enc_data$Package_Product_num))

# Normalizing the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

features <- subset(enc_data, select = -High_Ticket)
maxmindf_features <- as.data.frame(lapply(features, normalize))

High_Ticket <- enc_data$High_Ticket
maxmindf <- cbind(High_Ticket, maxmindf_features)

# Split into train and test sets
set.seed(36)
smpl <- sample.int(n = nrow(maxmindf), size = floor(0.8 * nrow(maxmindf)), replace = FALSE)
train_maxmin <- maxmindf[smpl, ]
test_maxmin <- maxmindf[-smpl, ]

# Modeling
model_maxmin <- neuralnet(High_Ticket ~ ., data = train_maxmin, hidden = c(3, 8, 5), linear.output = FALSE)


# Predicting
pred_maxmin <- predict(model_maxmin, test_maxmin)

# Testing the Accuracy of the models
maxmin_results <- data.frame(actual = test_maxmin$High_Ticket, prediction = pred_maxmin)

```

## Purpose and Target

When considering using a Neural Network, we decided to identify potential high value properties within the dataset. To do this, we engineered a new, binary column, identifying whether a property was considered "High-Ticket" or not. In order to categorize properties this way, we calculated the price per meter squared of each property, and those in the top quartile had their High_Ticket column value set to 1, others to 0.

With this transformation, we had created a binary classification problem: is this property a high-ticket property? Answering this question will allow us to filter out most properties which may not be as lucrative for Zug Estates, and offer only the cream of the crop as possible targets for acquisition.

## Interpretation

After classifying the properties into either category, we were able to analyze the model's performance on the dataset. If successful, this Neural Network will be a great tool in the future to predict a possible acquisition's performance over time. Properties may be acquired at lower prices, and may not be very attractive at the time of purchase, but with some remodeling and updating the living space, its perceived value may be brought to a new high. This is what we try to predict with this model: the potential of a specific property and to discern if it could be a lucrative investment for Zug Estates.


```{r NN Confusion Matrix, echo=FALSE, warning = FALSE, class.source = "fold-show"}
predicted_classes <- ifelse(pred_maxmin > 0.5, 1, 0)
actual_classes <- test_maxmin$High_Ticket
conf_matrix <- table(predicted_classes, actual_classes)

dimnames(conf_matrix) <- list(Predicted = c("Predicted: Non-High Ticket", "Predicted: High Ticket"),
                              Actual = c("Actual: Non-High Ticket", "Actual: High-Ticket"))

conf_matrix_df_nn <- as.data.frame.matrix(conf_matrix)

kbl(conf_matrix_df_nn, align="c") %>%
  kable_classic(full_width=F, html_font="Segoe UI") %>%
  kable_styling(latex_options = c("striped", "hold_position"), position="center", font_size=10) %>%
  column_spec(2:ncol(conf_matrix_df_nn), width="4cm") %>%
  row_spec(0, background = "#9C8AE6", color="white", bold=TRUE)
```
*Confusion Matrix of the Neural Network's performance.*

As seen in the previous matrix, the model's performance on categorizing properties into possible high-ticket or not is quite good, with only a total of 7 (3.6%) incorrect classifications. This tool will help in the future, when looking at new properties, to have an idea of more or less how lucrative a specific property could be to Zug Estates' portfolio.

```{r NN Predictions Chart, echo=FALSE}
# Convert to a data frame for ggplot2
rounded_predictions <- round(pred_maxmin)
data <- data.frame(predictions = factor(rounded_predictions, levels = c(0, 1), labels = c("Non-High Ticket", "High-Ticket")))

# Histogram
ggplot(data, aes(x = predictions, fill = predictions)) +
  geom_bar(stat = "count", color = "black", width = 0.5) +  # Use 'stat = "count"' to ensure it counts occurrences
  scale_x_discrete(name = "Predicted Class") +
  ylab("Frequency") +
  ggtitle("Rounded Predictions") +
  scale_fill_manual(values = c("Non-High Ticket" = "skyblue", "High-Ticket" = "#9C8AE6")) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 11, face = "bold"),
    axis.title = element_text(size = 10)
  )

```

*Predictions have been rounded because some values fell into decimal places, due to the nature of the Neural Networks' activation function (sigmoid curve)*

As we can see, not many values actually fall into the category of High-Ticket properties, so having a tool like this network will be beneficial in catching such opportunities early on and maximizing added value and profits for the real estate company.

```{r NN Predictions Visual, echo=FALSE}
ggplot(maxmin_results, aes(x = actual, y = prediction)) +
  geom_jitter(width = 0.1, height = 0.1, color = "#9C8AE6", alpha = 0.5) +  # Jitter to avoid overplotting
  labs(title = "Test Data | Actual vs Predicted", 
       x = "Actual", 
       y = "Predicted") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 11, face = "bold"),
    axis.title = element_text(size = 10)
  )
```

*In this visual we can better appreciate the behavior of the model's predictions. Jitter has been added to the values to avoid overplotting (All values on a single coordinate)*

As we can see, the clusters near (0,0) and (1,1) show us correct predictions made by the model. The small cluster on the bottom right (near (1,0)) shows us the mistakes. This graph gives us a similar representation to the confusion matrix, but also represents a visual estimate to prediction errors. We can see from the clearly defined clusters that our model is very confident in its predictions, and not many incorrect predictions were made.

```{r NN Decision Boundary, echo = FALSE}
# Define the range for Size_m2 and Price_Gross
size_m2_range <- seq(min(train_maxmin$Size_m2), max(train_maxmin$Size_m2), length.out = 100)
price_gross_range <- seq(min(train_maxmin$Price_Gross), max(train_maxmin$Price_Gross), length.out = 100)

# Create a grid of points
grid <- expand.grid(Size_m2 = size_m2_range, Price_Gross = price_gross_range)

# Add other features with their mean values
for (col in setdiff(colnames(train_maxmin), c("Size_m2", "Price_Gross", "High_Ticket"))) {
  grid[[col]] <- mean(train_maxmin[[col]])
}

# Predict the class for each point in the grid using the trained model
grid$prediction <- predict(model_maxmin, grid)
grid$prediction_class <- ifelse(grid$prediction > 0.5, 1, 0)

# Plot the actual data points and the decision boundary
ggplot() +
  geom_point(data = train_maxmin, aes(x = Size_m2, y = Price_Gross, color = as.factor(High_Ticket)), alpha = 0.6) +
  scale_color_manual(values = c("0" = "skyblue", "1" = "#9C8AE6"), name = "Property Type",
                     labels=c("Non-High Ticket", "High-Ticket")) +
  labs(title = "Decision Boundary of the Neural Network Model",
       x = "Size (m^2)", 
       y = "Rental Price (CHF)") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 11, face = "bold"),
    axis.title = element_text(size = 10)
  )


```
_Values have been normalized between 0 and 1 to allow for better visualization_

Here we can more clearly appreciate where the model has drawn its decision boundary. With this information, we conclude that the neural network model has clearly understood how to separate the properties between High-Ticket and not. In the purple section, we see the models classified as High-Ticket, which as is displayed, tend to be those with a higher "density" of price per square meter. For Zug Estates, this line represents the boundary between properties that may yield better financial results (purple) and those that may not be so profitable (blue). The fact that we can clearly distinguish them means that the model can as well, and that it can recognize a clear difference between possible high-ticket properties and those that may be more mediocre.

## Conclusion

A Neural Network proves to be a very versatile tool when it comes to adapting to data. This may be beneficial in the future, if this tool is to be implemented and retrained/exposed to new properties in order to provide some suggestions as to which to consider acquiring.

Having a tool such as this in Zug Estates' arsenal will most definitely provide an edge over the competition, as identifying higher value candidates for purchase will become faster and more efficient, helping the team close profitable deals in a quicker fashion.

<br>

# Support Vector Machine

## Purpose and Target

Though the Neural Network presented great results, we wanted to make sure to validate that it was the optimal choice for classification in our case. Support Vector Machines (SVMs henceforth) also excel at binary classification problems, so we wanted to create one and train it on the same data in order to visualize and compare performance, to ensure we use the better performing model for recommending properties to acquire.

In short, we want to make sure we offer Zug Estates the best option when it comes to predicting performance of possible acquisitions. Our effort comes to life by being thorough with our research, and comparing different models to suggest the better one for the problem. In this case, we compare the ability of a Neural Network and a SVM to recognize potential high-value properties.

```{r SVM Intro, echo=FALSE, results='hide'}

# Reading in data
ohe_data <- read_excel("data/data_cleaned/data_total_model_one_hot_encoded.xlsx")

#Feature Engineering
ohe_data$Price_per_m2 <- ohe_data$Price_Gross / ohe_data$Size_m2
q3 <- median(ohe_data$Price_per_m2) + IQR(ohe_data$Price_per_m2) / 2
ohe_data$High_Ticket <- ohe_data$Price_per_m2 > q3
ohe_data$High_Ticket <- ifelse(ohe_data$High_Ticket, 1, 0)

# Price_per_m2 causes collinearity issues
# Nr_rooms.8 has only one positive value, so we remove the row
# Nr_rooms.10 is constant, so we remove it as well
ohe_data <- ohe_data[, !names(ohe_data) %in% c("Price_per_m2","Nr_rooms.8",
                                               "Nr_rooms.10")]

# Convert target to factor since this is a classification problem (binary)
ohe_data$High_Ticket <- as.factor(ohe_data$High_Ticket)

# Random seed for reproducibility
set.seed(123) # same seed as NN for reproduction/comparison

# Split the data into training and test sets
train_index <- sample(seq_len(nrow(ohe_data)), size = floor(0.8 * nrow(ohe_data)))
train_data <- ohe_data[train_index, ]
test_data <- ohe_data[-train_index, ]

# Set up train control for cross-validation
train_control <- trainControl(
  method = "cv",         # Cross-validation
  number = 10,           # Number of folds
  savePredictions = "final",
  classProbs = TRUE,     # If you want class probabilities
  summaryFunction = twoClassSummary
)


# Train the SVM model
svm_model <- svm(High_Ticket ~ ., data = train_data,trControl=train_contor,
                 kernel = "radial",
                 cost = 10, scale = TRUE,
                 probability=TRUE)

# Exclude the target variable from the test set
test_data_without_target <- test_data[, !names(test_data) %in% 'High_Ticket']


# Make predictions
svm_predictions <- predict(svm_model, newdata = test_data_without_target)

```
<br>

## Interpretation

**Neural Network Performance**

```{r Re-NN Performance, echo=FALSE, warning = FALSE, class.source = "fold-show"}
kbl(conf_matrix_df_nn, align="c") %>%
  kable_classic(full_width=F, html_font="Segoe UI") %>%
  kable_styling(latex_options = c("striped", "hold_position"), position="center", font_size=10) %>%
  column_spec(2:ncol(conf_matrix_df_nn), width="4cm") %>%
  row_spec(0, background = "#9C8AE6", color="white", bold=TRUE)
```



**SVM Performance**

```{r SVM Confusion Matrix, echo=FALSE, warning = FALSE, class.source = "fold-show"}

conf_matrix_svm <- table(Predicted = svm_predictions, Actual = test_data$High_Ticket)

dimnames(conf_matrix_svm) <- list(Predicted = c("Predicted: Non-High Ticket", "Predicted: High Ticket"),
                              Actual = c("Actual: Non-High Ticket", "Actual: High-Ticket"))

conf_matrix_df_svm <- as.data.frame.matrix(conf_matrix_svm)

kbl(conf_matrix_df_svm, align="c") %>%
  kable_classic(full_width=F, html_font="Segoe UI") %>%
  kable_styling(latex_options = c("striped", "hold_position"), position="center", font_size=10) %>%
  column_spec(2:ncol(conf_matrix_df_svm), width="4cm") %>%
  row_spec(0, background = "#9C8AE6", color="white", bold=TRUE)

```

As we can see, comparing the two confusion matrices of both models, the Neural Network has  slightly better accuracy than the SVM. Though at first glance it may seem like the better option, there are several advantages and disadvantages between the models, which we will go over shortly.

```{r SVM ROC Curve, echo=FALSE, warning=FALSE}
svm_probabilities <- attr(predict(svm_model, newdata = test_data_without_target, probability = TRUE), "probabilities")[,2]

roc_curve <- roc(test_data$High_Ticket, svm_probabilities)

# Extract AUC value
auc_value <- auc(roc_curve)
auc_text <- paste("AUC:", round(auc_value, 4))

# Creating data frame for ROC curve
roc_data <- data.frame(
  fpr = 1 - rev(roc_curve$specificities),
  tpr = rev(roc_curve$sensitivities)
)

# Plotting the ROC curve
ggplot(roc_data, aes(x = fpr, y = tpr)) +
  geom_line(color = "#9C8AE6", size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "skyblue") +
  labs(title = "ROC Curve for SVM Model",
       x = "False Positive Rate",
       y = "True Positive Rate") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 11, face = "bold"),
    axis.title = element_text(size = 11)
  ) +
  annotate("text", x = 0.6, y = 0.1, label = auc_text, color = "black", size = 5, hjust = 0)
```

An SVM is particularly effective when working with higher-dimensional data, and has better memory efficiency. It is also more robust to over-fitting, but are less efficient with larger datasets because of the training complexity. In our case however, we interpret the ROC Curve's shape as well as the are-under-curve (AUC = 0.9885) to be excellent. The SVM is adapting very well to the data, and has very strong discriminative power.

Neural networks on the other hand, are more flexible and powerful when it comes to approximating trends. They can also learn and extract features that are not present in the data, finding new relationships that may not be obvious, but can be very computationally intensive.


```{r SVM Metrics Table, echo=FALSE}
conf_matrix_svm <- confusionMatrix(svm_predictions, test_data$High_Ticket, positive="1")

# Extract Metrics
acc <- conf_matrix_svm$overall["Accuracy"]
prec <- conf_matrix_svm$byClass["Pos Pred Value"]
rec <- conf_matrix_svm$byClass["Sensitivity"]
f1_score <- conf_matrix_svm$byClass["F1"]

metrics_df <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall", "F1-Score"),
  Value = c(acc, prec, rec, f1_score)
)

metrics_tibble <- as_tibble(metrics_df)

kbl(metrics_tibble, align="c") %>%
  kable_classic(full_width=F, html_font = "Segoe UI") %>%
  kable_styling(latex_options = c("striped", "hold_position"), position = "center", font_size=10) %>%
  column_spec(1, bold=TRUE, width="3cm") %>%
  column_spec(2, width="2cm")%>%
  row_spec(0, background = "#9C8AE6", color = "white", bold=TRUE)

```
_SVM Metrics_

In the table above, we are able to see how the model is performing in a more numerical sense. The high accuracy (94.82%) and precision (95.24%) values suggest that our model is not making many mistakes when predicting on the test data, and is very good a recognizing positive values (High Ticket). The recall score (83.33%) and F1 score (88.89%) suggest that the model is capturing most of the positive instances, and also has a good balance between precision and recall, which suggests the model is effective at minimizing false positives.

## Conclusion

In summary, both the NN and SVM are good options for our purpose. In the future, if considering to use either one, the size, shape and complexity of the available dataset will play a pivotal role in the selection of the best model to use for finding possible "golden goose" properties. This is not, however, meant to be interpreted as: "There is only one right answer". As a suggestion, when faced with a list of properties to choose from, both models may be used to gain valuable insight, and both would provide similar results with some slight variations, which can only be interpreted then.

<br>

# Outlook

We would like to highlight our top four recommendations for improvements in future endeavors related to this project and more broadly in the field of employee attrition.

## Data Quality and Amount

Future studies could benefit from investigating ensemble methods, which merge the advantages of various models to boost overall efficacy. We suggest that a larger dataset might enhance the performance of specific models like neural networks. Furthermore, gathering additional predictive variables could facilitate the training of more sophisticated models.

The primary focus of our project was centered on identifying general predictors of attrition. This objective shaped our analytical approach and the methodologies we employed, and provided a foundation for understanding the key predictors. An extension of our analysis would be to change the focus and predict the probabilities of employee attrition, rather than solely classifying outcomes. This approach would enable us to personalize interventions more effectively, tailoring strategies to the specific likelihood of an employee’s departure.

## Enhanced Validation of Models

Due to the time constraints and scope of this project, we couldn’t conduct extensive validation and testing for all models. Implementing more techniques such as cross-validation and regularization in the training phase would be necessary to enhance the overall quality and performance of the developed models. Moreover, testing interactions was at short-cut in this project. In

## Model Complexity vs. Performance

The study underscored the delicate balance between model complexity and performance. Although more sophisticated models, as seen in the GAM and SVM sections, are adept at identifying complex patterns, they don’t always guarantee the most accurate predictions in every situation, particularly regarding specificity. When developing a model for a company, it’s essential to clearly articulate the model’s intended purpose and application to determine the most appropriate solution.

## Data Privacy and Ethics

While we have pointed out that various stakeholders can gain from these models, it’s crucial to note that the personal data used here, such as marital status, is highly sensitive in non-fictional datasets. Companies should thoughtfully consider which data points to utilize about their employees. Relying on certain predictors can potentially introduce significant bias, particularly against underrepresented groups within the organization.

<br>

# Personal Learnings

This project was a great opportunity for us to learn about machine learning and develop our skills. We really enjoyed experimenting with different models, which is why our report is so long - we had a lot of work to cut down.

Our models are of solid quality, but we didn’t get to explore every detail or use all the methods perfectly. For example, our early graphical analysis could be improved by using methods better suited to different types of data than just correlation coefficients, or validation of different models is not fully implemented (CSV). The neural network component, in particular, posed significant challenges. Our results here were not as robust as we had hoped, possibly due to limitations in the amount of data available. This experience highlighted the importance of having a substantial dataset for neural network models to truly excel.

An important lesson we learned was the need to establish common measures in advance for effective model comparison. This ensures that a meaningful comparison can be made using the measures. We also learned the importance of considering the data set when choosing metrics. In our case, relying solely on accuracy as a metric wasn’t appropriate due to the unbalanced nature of the dataset.

## Implementation of Generative AI

It’s important to note that OpenAI’s Chat-GPT and Google’s Gemini LLMs were valuable resources when we were faced with difficult debugging problems.

However, we made a point of understanding, modifying and correcting the suggested code independently to ensure that our final work was truly our own and not a mere replication of the solutions provided.

These LLMs were also a great aid when creating interesting visual aids and clarifying insights. Their help not only made our understanding of machine learning models better, but furthered our understanding and use of R as a statistical programming language and tool.

In the end, we’re happy with the end result of our report, and even happier with how much we learned along the way.

## Extra

Our team sent this project proposal to the investor relations email at Zug Estates a week ago, driven by the thought that it might be interesting to reach out given our current search for internships in data science with their esteemed company. We are thrilled to share that Zug Estates has responded positively, expressing that the concept is highly intriguing and that they would like to meet us. We are excited to announce that we will be meeting at their offices in the heart of Zug to discuss this further in the week of the 24th of June.

<br>

# References
